{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from huggingface_hub import hf_hub_download\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import numpy as np\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Optional\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from circuits.dictionary_learning.dictionary import AutoEncoder, AutoEncoderNew, GatedAutoEncoder, IdentityDict\n",
    "from circuits.utils import (\n",
    "    othello_hf_dataset_to_generator,\n",
    "    get_model,\n",
    "    get_submodule,\n",
    "    get_mlp_activations_submodule,\n",
    ")\n",
    "\n",
    "from feature_viz_othello_utils import (\n",
    "    get_acts_IEs_VN,\n",
    "    plot_lenses,\n",
    "    plot_mean_metrics,\n",
    "    plot_top_k_games,\n",
    "    BoardPlayer,\n",
    ")\n",
    "\n",
    "import circuits.utils as utils\n",
    "import circuits.analysis as analysis\n",
    "import feature_viz_othello_utils as viz_utils\n",
    "import circuits.othello_utils as othello_utils\n",
    "from circuits.othello_engine_utils import to_board_label, to_string, to_int, stoi_indices #to_string: mode_output_vocab to interpretable square index\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n",
    "\n",
    "device = 'cuda:0'\n",
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "repo_dir = '/share/u/can/chess-gpt-circuits'\n",
    "repo_dir = \"/home/adam/chess-gpt-circuits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model, submodule, ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Baidicoot/Othello-GPT-Transformer-Lens\"\n",
    "model = get_model(model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 5\n",
    "# node_type = \"sae_feature\"\n",
    "node_type = \"mlp_neuron\"\n",
    "\n",
    "\n",
    "if node_type == \"sae_feature\":\n",
    "    ae_group_name = 'all_layers_othello_p_anneal_0530_with_lines'\n",
    "    ae_type = 'p_anneal'\n",
    "    trainer_id = 0\n",
    "    ae_path = f'{repo_dir}/autoencoders/{ae_group_name}/layer_{layer}/trainer{trainer_id}'\n",
    "    submodule = get_submodule(model_name, layer, model)\n",
    "elif node_type == \"mlp_neuron\":\n",
    "    ae_group_name = 'othello_mlp_acts_identity_aes_lines' # with_lines\n",
    "    ae_type = 'identity'\n",
    "    ae_path = f'{repo_dir}/autoencoders/{ae_group_name}/layer_{layer}'\n",
    "    submodule = get_mlp_activations_submodule(model_name, layer, model)\n",
    "else:\n",
    "    raise ValueError('Invalid node_type')\n",
    "\n",
    "# download data from huggingface if needed\n",
    "if not os.path.exists(f'{repo_dir}/autoencoders/{ae_group_name}'):\n",
    "    hf_hub_download(repo_id='adamkarvonen/othello_saes', filename=f'{ae_group_name}.zip', local_dir=f'{repo_dir}/autoencoders')\n",
    "    # unzip the data\n",
    "    os.system(f'unzip {repo_dir}/autoencoders/{ae_group_name}.zip -d {repo_dir}/autoencoders')\n",
    "\n",
    "# Initialize the autoencoder\n",
    "if ae_type == 'standard' or ae_type == 'p_anneal':\n",
    "    ae = AutoEncoder.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "elif ae_type == 'gated' or ae_type == 'gated_anneal':\n",
    "    ae = GatedAutoEncoder.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "elif ae_type == 'standard_new':\n",
    "    ae = AutoEncoderNew.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "elif ae_type == 'identity':\n",
    "    ae = IdentityDict()\n",
    "else:\n",
    "    raise ValueError('Invalid ae_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load legal move neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each neuron, we need the full hypothesis [feature_idx, valid_square_idx, [config_idxs]]\n",
    "import json\n",
    "with open(os.path.join(ae_path, 'hpc_hrc_same_square_indexes_dict.json'), 'r') as f:\n",
    "    hpc_hrc_same_square_indexes_dict = json.load(f)\n",
    "\n",
    "print(hpc_hrc_same_square_indexes_dict.keys())\n",
    "print(hpc_hrc_same_square_indexes_dict['high_precision_and_recall'])\n",
    "print(hpc_hrc_same_square_indexes_dict['intersection_FSqC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_idxs, valid_square_idxs, line_idxs = t.tensor(hpc_hrc_same_square_indexes_dict['intersection_FSqC']).T\n",
    "unique_feat_idxs = t.unique(feat_idxs)\n",
    "unique_valid_square_idxs = t.unique(valid_square_idxs)\n",
    "n_unique_feat_idxs = len(unique_feat_idxs)\n",
    "\n",
    "for feat_idx, square_idx in zip(unique_feat_idxs, unique_valid_square_idxs):\n",
    "    print(feat_idx, square_idx)\n",
    "    config_idxs_per_feat = line_idxs[feat_idxs == feat_idx]\n",
    "    feat_idx = int(feat_idx)\n",
    "    square_idx = int(square_idx)\n",
    "    if feat_idx > 500:\n",
    "        break\n",
    "\n",
    "print(feat_idx, square_idx, config_idxs_per_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 500\n",
    "\n",
    "ablation_dataset_name = \"othello_ablation_dataset.pkl\"\n",
    "\n",
    "if os.path.exists(ablation_dataset_name):\n",
    "    print(\"Loading ablation dataset\")\n",
    "    with open(ablation_dataset_name, \"rb\") as f:\n",
    "        ablation_data = pickle.load(f)\n",
    "else:\n",
    "    ablation_data = eval_sae.construct_othello_dataset(\n",
    "        custom_functions=[\n",
    "            othello_utils.games_batch_to_state_stack_length_lines_mine_BLRRC,\n",
    "            othello_utils.games_batch_to_valid_moves_BLRRC,\n",
    "        ],\n",
    "        n_inputs=dataset_size,\n",
    "        split=\"train\",\n",
    "        precompute_dataset=True,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"Saving ablation dataset\")\n",
    "    with open(ablation_dataset_name, \"wb\") as f:\n",
    "        pickle.dump(ablation_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_any_present(train_data, valid_move_idx, line_idxs):\n",
    "    '''for every position in every game, evaluate whether any of the lines is present'''\n",
    "    r = valid_move_idx // 8\n",
    "    c = valid_move_idx % 8\n",
    "    data_BLC = train_data['games_batch_to_state_stack_length_lines_mine_BLRRC'][:, :, r, c, :]\n",
    "    B, L, C = data_BLC.shape\n",
    "\n",
    "    lines_C = t.zeros(C, dtype=t.int64)\n",
    "    lines_C[line_idxs] = 1\n",
    "\n",
    "    data_with_lines_BLC = data_BLC * lines_C\n",
    "    any_line_present_BL = einops.reduce(data_with_lines_BLC, 'b L C -> b L', 'sum') > 0\n",
    "    return any_line_present_BL\n",
    "\n",
    "def check_valid_move(train_data: dict, valid_move_idx: int) -> t.Tensor:\n",
    "    '''for every position in every game, evaluate whether there is a valid move on the square'''\n",
    "    r = valid_move_idx // 8\n",
    "    c = valid_move_idx % 8\n",
    "    data_BL1 = train_data['games_batch_to_valid_moves_BLRRC'][:, :, r, c]\n",
    "    data_BL = data_BL1.squeeze().bool()\n",
    "    return data_BL\n",
    "\n",
    "def game_state_where_line_present(train_data, valid_move_idx, line_idxs):\n",
    "    any_line_present = check_any_present(train_data, valid_move_idx, line_idxs)\n",
    "    enc_inputs = t.tensor(train_data['encoded_inputs'], device=device)\n",
    "    game_state_where_line_present = []\n",
    "    for game, line_present in zip(enc_inputs, any_line_present):\n",
    "        if line_present.sum() > 0:\n",
    "            first_occurence = t.where(line_present)[0][0]\n",
    "            game_state_where_line_present.append(game[:first_occurence+1])\n",
    "    return game_state_where_line_present\n",
    "\n",
    "square_test = check_valid_move(ablation_data, square_idx)\n",
    "print(square_test.shape)\n",
    "\n",
    "# def game_state_my_move_before_line_present(train_data, valid_move_idx, line_idxs):\n",
    "#     game_state_where_line_present = game_state_where_line_present(train_data, valid_move_idx, line_idxs)\n",
    "#     return [game[:-2] for game in game_state_where_line_present]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "game_states_line_present = game_state_where_line_present(ablation_data, square_idx, config_idxs_per_feat)\n",
    "game = game_states_line_present[1]\n",
    "game = t.tensor(game, device=device)\n",
    "player = BoardPlayer(game)\n",
    "\n",
    "square_idx, config_idxs_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1 game, where any line is present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def steering(model, game_batch, submodule, ae, feat_idx, square_idx, steering_factor, device='cpu'):\n",
    "    \n",
    "\n",
    "#     for game, V_square_idx, V_rotated_square_idx, C_square_idx in zip(game_batch):\n",
    "#         V_square_idx = square_idx\n",
    "#     V_row = V_square_idx // 8\n",
    "#     V_col = V_square_idx % 8\n",
    "#     V_rotated_row = 7 - V_row\n",
    "#     V_rotated_col = 7 - V_col\n",
    "#     V_rotated_square_idx = V_rotated_row * 8 + V_rotated_col\n",
    "#     C_square_idxs = game[-2]\n",
    "#     # Clean forward pass\n",
    "#     with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "#         x = submodule.output\n",
    "#         f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#         logits_clean = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#     steering_value = f[:, -1, feat_idx] # Activation value where valid_move is present\n",
    "    \n",
    "#     # Steering forward pass\n",
    "#     with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "#         x = submodule.output\n",
    "#         f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#         f[:, :, feat_idx] = steering_value * steering_factor\n",
    "#         submodule.output = ae.decode(f)\n",
    "#         logits_steer = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#     # Logit diffs for t-2\n",
    "#     arange_batch = t.arange(batch_size, device=device)\n",
    "#     logit_diff_clean = logits_clean[arange_batch, -3, to_int(V_square_idxs)] - logits_clean[arange_batch, -3, to_int(C_square_idxs)]\n",
    "#     logit_diff_steer = logits_steer[arange_batch, -3, to_int(V_square_idxs)] - logits_steer[arange_batch, -3, to_int(C_square_idxs)]\n",
    "\n",
    "#     rotated_logit_diff_clean = logits_clean[arange_batch, -3, to_int(V_rotated_square_idxs)] - logits_clean[arange_batch, -3, to_int(C_square_idxs)]\n",
    "#     rotated_logit_diff_steer = logits_steer[arange_batch, -3, to_int(V_rotated_square_idxs)] - logits_steer[arange_batch, -3, to_int(C_square_idxs)]\n",
    "    \n",
    "#     steer_clean_diff = logit_diff_steer - logit_diff_clean\n",
    "#     rotated_steer_clean_diff = rotated_logit_diff_steer - rotated_logit_diff_clean\n",
    "#     return steer_clean_diff, rotated_steer_clean_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def steering(model, game_batch, submodule, ae, feat_idx, square_idx, steering_factor, timestep=-2, device='cpu'):\n",
    "#     batch_size = len(game_batch)\n",
    "#     steer_clean_diffs = t.zeros(batch_size, device=device)\n",
    "#     rotated_steer_clean_diffs = t.zeros(batch_size, device=device)\n",
    "#     boards_clean = t.zeros(batch_size, 64, device=device)\n",
    "#     boards_steer = t.zeros(batch_size, 64, device=device)\n",
    "\n",
    "#     for i, game in tqdm(enumerate(game_batch), desc='Steering Batch', total=batch_size):\n",
    "#         V_square_idx = square_idx\n",
    "#         V_row = V_square_idx // 8\n",
    "#         V_col = V_square_idx % 8\n",
    "#         V_rotated_row = 7 - V_row\n",
    "#         V_rotated_col = 7 - V_col\n",
    "#         V_rotated_square_idx = V_rotated_row * 8 + V_rotated_col\n",
    "#         C_square_idx = game[-2]\n",
    "\n",
    "#         # Clean forward pass\n",
    "#         with t.no_grad(), model.trace(game, **tracer_kwargs):\n",
    "#             x = submodule.output\n",
    "#             f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#             logits_clean = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#         steering_value = f[:, -1, feat_idx] # Activation value where valid_move is present\n",
    "        \n",
    "#         # Steering forward pass\n",
    "#         with t.no_grad(), model.trace(game, **tracer_kwargs):\n",
    "#             x = submodule.output\n",
    "#             f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#             f[:, :, feat_idx] = steering_value * steering_factor\n",
    "#             submodule.output = ae.decode(f)\n",
    "#             logits_steer = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#         # Logit diffs for t-2\n",
    "#         logit_diff_clean = logits_clean[:, timestep, to_int(V_square_idx)] - logits_clean[:, timestep, to_int(C_square_idx)]\n",
    "#         logit_diff_steer = logits_steer[:, timestep, to_int(V_square_idx)] - logits_steer[:, timestep, to_int(C_square_idx)]\n",
    "\n",
    "#         rotated_logit_diff_clean = logits_clean[:, timestep, to_int(V_rotated_square_idx)] - logits_clean[:, timestep, to_int(C_square_idx)]\n",
    "#         rotated_logit_diff_steer = logits_steer[:, timestep, to_int(V_rotated_square_idx)] - logits_steer[:, timestep, to_int(C_square_idx)]\n",
    "    \n",
    "#         steer_clean_diffs[i] = logit_diff_steer - logit_diff_clean\n",
    "#         rotated_steer_clean_diffs[i] = rotated_logit_diff_steer - rotated_logit_diff_clean\n",
    "#         boards_clean[i][stoi_indices] = logits_clean[0, timestep, 1:]\n",
    "#         boards_steer[i][stoi_indices] = logits_steer[0, timestep, 1:]\n",
    "\n",
    "#     return steer_clean_diffs, rotated_steer_clean_diffs, boards_clean, boards_steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square_idx, config_idxs_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_batch_example = game_state_where_line_present(train_data_example, square_idx, config_idxs_per_feat)\n",
    "# print('Number of games where line is present:', len(game_batch_example))\n",
    "# for steering_factor in [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]:\n",
    "#     steer_clean_diff, rotated_steer_clean_diff, boards_clean, boards_steer = steering(\n",
    "#         model, \n",
    "#         game_batch_example, \n",
    "#         submodule, \n",
    "#         ae, \n",
    "#         feat_idx, \n",
    "#         square_idx, \n",
    "#         steering_factor,\n",
    "#         timestep=-1,\n",
    "#         device=device\n",
    "#         )\n",
    "#     steer_clean_diff = steer_clean_diff.mean()\n",
    "#     rotated_steer_clean_diff = rotated_steer_clean_diff.mean()\n",
    "#     print(f\"Steering factor: {steering_factor}, steer_clean_diff: {steer_clean_diff.item() - rotated_steer_clean_diff.item()}, rotated_steer_clean_diff: {rotated_steer_clean_diff.item()}\")\n",
    "#     # plt.imshow(boards_clean[3].view(8, 8).cpu().numpy() - boards_steer[0].view(8, 8).cpu().numpy())\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single batch of size 100\n",
    "track number of times the condition is present\n",
    "\n",
    "do clean forward pass\n",
    "do mean ablation / zero ablation forward pass.\n",
    "\n",
    "compute difference in IEs. \n",
    "Do Activation patching for HRC and HPC neurons first\n",
    "Test wheter activation patching is also feasible for..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_move_neurons = hpc_hrc_same_square_indexes_dict[\"high_precision_and_recall\"]\n",
    "\n",
    "\n",
    "def activation_patching(\n",
    "    model,\n",
    "    train_data: dict,\n",
    "    submodule,\n",
    "    ae,\n",
    "    feat_idxs: t.Tensor,\n",
    "    square_idx: int,\n",
    "    config_idxs_per_feat,\n",
    "    ablation_method=\"zero\",\n",
    "    device=\"cpu\",\n",
    "    filter_for_valid_moves: bool = False,\n",
    "):\n",
    "    assert ablation_method in [\n",
    "        \"mean\",\n",
    "        \"zero\",\n",
    "    ], \"Invalid ablation method. Must be one of ['mean', 'zero']\"\n",
    "    game_batch = t.tensor(train_data[\"encoded_inputs\"])\n",
    "\n",
    "    # Get clean logits and mean submodule activations\n",
    "    with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "        x = submodule.output\n",
    "        if ablation_method == \"mean\":\n",
    "            f = ae.encode(x)  # shape: [batch_size, seq_len, n_features]\n",
    "            f_mean_clean = f.mean(dim=(0, 1))\n",
    "        logits_clean_BLV = model.unembed.output.save()  # batch_size x seq_len x vocab_size\n",
    "\n",
    "    # Get patch logits\n",
    "    with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "        x = submodule.output\n",
    "        f = ae.encode(x)\n",
    "        if ablation_method == \"mean\":\n",
    "            f[:, :, feat_idx] = f_mean_clean[feat_idx]\n",
    "        else:\n",
    "            f[:, :, feat_idxs] = 0\n",
    "        submodule.output = ae.decode(f)\n",
    "        logits_patch_BLV = model.unembed.output.save()\n",
    "\n",
    "    logit_diff_BLV = logits_patch_BLV - logits_clean_BLV\n",
    "    logit_diff_BL = logit_diff_BLV[:, :, to_int(square_idx)]\n",
    "\n",
    "    probs_clean_BLV = t.nn.functional.softmax(logits_clean_BLV, dim=-1)\n",
    "    probs_patch_BLV = t.nn.functional.softmax(logits_patch_BLV, dim=-1)\n",
    "\n",
    "    probs_diff_BLV = probs_patch_BLV - probs_clean_BLV\n",
    "    probs_diff_BL = probs_diff_BLV[:, :, to_int(square_idx)]\n",
    "\n",
    "    if filter_for_valid_moves:\n",
    "        # Filter for valid moves on square_idx\n",
    "        custom_mask_BL = check_valid_move(train_data, square_idx)\n",
    "    else:\n",
    "        # Filter for a list of lines in config_idxs_per_feat\n",
    "        custom_mask_BL = check_any_present(train_data, square_idx, config_idxs_per_feat)\n",
    "\n",
    "\n",
    "    logit_diff_line_present_BL = logit_diff_BL[custom_mask_BL]\n",
    "    logit_diff_line_absent_BL = logit_diff_BL[~custom_mask_BL]\n",
    "\n",
    "    probs_diff_line_present_BL = probs_diff_BL[custom_mask_BL]\n",
    "    probs_diff_line_absent_BL = probs_diff_BL[~custom_mask_BL]\n",
    "\n",
    "    return (\n",
    "        logit_diff_line_present_BL,\n",
    "        logit_diff_line_absent_BL,\n",
    "        custom_mask_BL,\n",
    "        probs_diff_line_present_BL,\n",
    "        probs_diff_line_absent_BL,\n",
    "        logits_clean_BLV[:, :, :],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_idx, square_idx, config_idxs_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hpc_hrc_same_square_indexes_dict.keys())\n",
    "print(hpc_hrc_same_square_indexes_dict['intersection_FSqC'])\n",
    "print(hpc_hrc_same_square_indexes_dict['high_precision_and_recall'])\n",
    "print(len(hpc_hrc_same_square_indexes_dict['intersection_FSqC']))\n",
    "print(len(hpc_hrc_same_square_indexes_dict['high_precision_and_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_feat_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ablation_data.keys())\n",
    "print(ablation_data['encoded_inputs'])\n",
    "print(ablation_data['games_batch_to_state_stack_length_lines_mine_BLRRC'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "n_batches = 10\n",
    "\n",
    "# feat_idxs, valid_square_idxs, line_idxs = t.tensor(hpc_hrc_same_square_indexes_dict['intersection_FSqC']).T\n",
    "# unique_feat_idxs = t.unique(feat_idxs)\n",
    "# unique_valid_square_idxs = t.unique(valid_square_idxs)\n",
    "# n_unique_feat_idxs = len(unique_feat_idxs)\n",
    "\n",
    "# print(unique_valid_square_idxs)\n",
    "\n",
    "# count = 0\n",
    "# for feat_idx, square_idx in zip(unique_feat_idxs, unique_valid_square_idxs):\n",
    "#     config_idxs_per_feat = line_idxs[feat_idxs == feat_idx]\n",
    "#     feat_idx = int(feat_idx)\n",
    "#     square_idx = int(square_idx)\n",
    "#     count += 1\n",
    "#     if count > 10:\n",
    "#         break\n",
    "#     if feat_idx > 500:\n",
    "#         break\n",
    "# square_idx = 13\n",
    "\n",
    "\n",
    "def extract_batch(data, batch_idx, batch_size):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch_data = {\n",
    "        key: (\n",
    "            value[start_idx:end_idx]\n",
    "            if isinstance(value, list)\n",
    "            else value[start_idx:end_idx].clone()\n",
    "        )\n",
    "        for key, value in data.items()\n",
    "    }\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "def get_all_neurons_for_square(intersection_FSqC: list, square_of_interest: int) -> t.Tensor:\n",
    "    unique_neurons = set()\n",
    "    for feat_idx, square_idx, config_idxs_per_feat in intersection_FSqC:\n",
    "        if square_idx == square_of_interest:\n",
    "            unique_neurons.add(feat_idx)\n",
    "    return list(unique_neurons)\n",
    "\n",
    "def get_square_for_neuron(intersection_FSqC: list, feat_idx: int) -> int:\n",
    "    for feat_idx_, square_idx, config_idxs_per_feat in intersection_FSqC:\n",
    "        if feat_idx_ == feat_idx:\n",
    "            return square_idx\n",
    "    raise ValueError(f\"Neuron {feat_idx} not found in intersection_FSqC\")\n",
    "\n",
    "\n",
    "def run_ablations(feat_idxs: t.Tensor, square_idx: int, config_idxs_per_feat: Optional[t.Tensor] = None):\n",
    "\n",
    "    logit_diffs_line_present = []\n",
    "    logit_diffs_line_absent = []\n",
    "    line_masks = []\n",
    "    probs_diffs_line_present = []\n",
    "    probs_diffs_line_absent = []\n",
    "\n",
    "    if config_idxs_per_feat is None:\n",
    "        filter_for_valid_moves = True\n",
    "    else:\n",
    "        filter_for_valid_moves = False\n",
    "\n",
    "    # Iterate over the number of batches\n",
    "    for batch_idx in range(n_batches):\n",
    "        train_data = extract_batch(ablation_data, batch_idx, batch_size)\n",
    "\n",
    "        (\n",
    "            logit_diff_line_present,\n",
    "            logit_diff_line_absent,\n",
    "            line_mask,\n",
    "            probs_diff_line_present,\n",
    "            probs_diff_line_absent,\n",
    "            logits_clean,\n",
    "        ) = activation_patching(\n",
    "            model,\n",
    "            train_data,\n",
    "            submodule,\n",
    "            ae,\n",
    "            feat_idxs,\n",
    "            square_idx,\n",
    "            config_idxs_per_feat,\n",
    "            ablation_method=\"zero\",\n",
    "            # ablation_method='mean',\n",
    "            device=device,\n",
    "            filter_for_valid_moves=filter_for_valid_moves,\n",
    "        )\n",
    "\n",
    "        if logit_diff_line_present.numel() > 0:\n",
    "            logit_diffs_line_present.append(logit_diff_line_present.mean())\n",
    "            probs_diffs_line_present.append(probs_diff_line_present.mean())\n",
    "        probs_diffs_line_absent.append(probs_diff_line_absent.mean())\n",
    "        logit_diffs_line_absent.append(logit_diff_line_absent.mean())\n",
    "        line_masks.append(line_mask.float().mean())\n",
    "\n",
    "        # print(f'batch {batch_idx}')\n",
    "        # print(f'mean logit diff for line present: {logit_diff_line_present.mean()}')\n",
    "        # print(f'mean logit diff for line absent: {logit_diff_line_absent.mean()}')\n",
    "        # print(f'fraction of line present: {line_mask.float().mean()}')\n",
    "\n",
    "    try:\n",
    "        print(f\"mean logit diff for condition present: {t.stack(logit_diffs_line_present).mean()}\")\n",
    "        print(f\"mean logit diff for condition absent: {t.stack(logit_diffs_line_absent).mean()}\")\n",
    "        print(f\"mean probability diff for condition present: {t.stack(probs_diffs_line_present).mean()}\")\n",
    "        print(f\"mean probability diff for condition absent: {t.stack(probs_diffs_line_absent).mean()}\")\n",
    "        print(f\"fraction of condition present: {t.stack(line_masks).mean()}\")\n",
    "    except:\n",
    "        print(\"No valid moves\")\n",
    "\n",
    "# Analyze every neuron with all lines per neuron\n",
    "for feat_idx in unique_feat_idxs:\n",
    "    square_idx = get_square_for_neuron(\n",
    "        hpc_hrc_same_square_indexes_dict[\"intersection_FSqC\"], feat_idx\n",
    "    )\n",
    "    config_idxs_per_feat = line_idxs[feat_idxs == feat_idx]\n",
    "    print(feat_idx, square_idx, config_idxs_per_feat)\n",
    "    run_ablations(feat_idx, square_idx, config_idxs_per_feat)\n",
    "\n",
    "# Analyze every square with all neurons per square\n",
    "# for square_idx in range(64):\n",
    "#     square_valid_move_neurons = get_all_neurons_for_square(\n",
    "#         hpc_hrc_same_square_indexes_dict[\"intersection_FSqC\"], square_idx\n",
    "#     )\n",
    "\n",
    "#     print(square_valid_move_neurons)\n",
    "\n",
    "#     print(f\"\\nFor square {square_idx}, there are {len(square_valid_move_neurons)} neurons\")\n",
    "#     if len(square_valid_move_neurons) > 0:\n",
    "#         run_ablations(t.tensor(square_valid_move_neurons), square_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
