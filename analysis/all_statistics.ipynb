{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Currently chess only) Dataframe comparing SAE statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import einops\n",
    "from datasets import load_dataset\n",
    "from typing import Callable, Optional\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from nnsight import NNsight\n",
    "import json\n",
    "from typing import Any\n",
    "from datasets import load_dataset\n",
    "from einops import rearrange\n",
    "from jaxtyping import Int, Float, jaxtyped\n",
    "from torch import Tensor\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from circuits.othello_buffer import OthelloActivationBuffer\n",
    "from circuits.dictionary_learning import AutoEncoder\n",
    "from circuits.chess_utils import encode_string\n",
    "from circuits.dictionary_learning import ActivationBuffer\n",
    "from circuits.dictionary_learning.dictionary import AutoEncoder, GatedAutoEncoder\n",
    "from circuits.dictionary_learning.trainers.gated_anneal import GatedAnnealTrainer\n",
    "from circuits.dictionary_learning.trainers.gdm import GatedSAETrainer\n",
    "from circuits.dictionary_learning.trainers.p_anneal import PAnnealTrainer\n",
    "from circuits.dictionary_learning.trainers.standard import StandardTrainer\n",
    "from circuits.dictionary_learning.evaluation import evaluate\n",
    "from circuits.nanogpt_to_hf_transformers import NanogptTokenizer, convert_nanogpt_model\n",
    "from circuits.eval_sae_as_classifier import (\n",
    "    initialize_results_dict, \n",
    "    get_data_batch, \n",
    "    apply_indexing_function,\n",
    "    construct_eval_dataset,\n",
    "    construct_othello_dataset,\n",
    "    prep_firing_rate_data,\n",
    ")\n",
    "from circuits.utils import (\n",
    "    get_model, \n",
    "    get_submodule,\n",
    "    get_ae_bundle,\n",
    "    collect_activations_batch,\n",
    "    get_nested_folders,\n",
    "    get_firing_features,\n",
    "    to_device,\n",
    "    AutoEncoderBundle,\n",
    ")\n",
    "import circuits.chess_utils as chess_utils\n",
    "import circuits.othello_utils as othello_utils\n",
    "import circuits.othello_engine_utils as othello_engine_utils\n",
    "\n",
    "from circuits.dictionary_learning.evaluation import evaluate\n",
    "\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "# Dimension key (from https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd):\n",
    "# F  = features and minibatch size depending on the context (maybe this is stupid)\n",
    "# B = batch_size\n",
    "# L = seq length (context length)\n",
    "# T = thresholds\n",
    "# R = rows (or cols)\n",
    "# C = classes for one hot encoding\n",
    "\n",
    "home_dir = '/share/u/can'\n",
    "repo_dir = f'{home_dir}/chess-gpt-circuits'\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "torch.set_grad_enabled(False)\n",
    "batch_size = 32\n",
    "feature_batch_size = batch_size\n",
    "n_inputs = 2048 # Length of the eval dataset\n",
    "GAME = \"chess\" # \"chess\" or \"othello\"\n",
    "\n",
    "models_path = repo_dir + \"/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and init game specific variables\n",
    "\n",
    "if GAME == \"chess\":\n",
    "    othello = False\n",
    "\n",
    "    autoencoder_group_paths = [\"/autoencoders/group1/\"]\n",
    "    custom_functions = [chess_utils.board_to_piece_state] #, chess_utils.board_to_pin_state]\n",
    "    model_name = \"adamkarvonen/8LayerChessGPT2\"\n",
    "    # data = construct_eval_dataset(custom_functions, n_inputs, models_path=models_path, device=DEVICE)\n",
    "    indexing_functions = [chess_utils.get_even_list_indices]\n",
    "\n",
    "elif GAME == \"othello\":\n",
    "    othello = True\n",
    "\n",
    "    autoencoder_group_paths = [\"/autoencoders/othello_layer0/\"]\n",
    "    # autoencoder_group_paths = [\"autoencoders/othello_layer0/\", \"autoencoders/othello_layer5_ef4/\"]\n",
    "    custom_functions = [\n",
    "            # othello_utils.games_batch_no_last_move_to_state_stack_BLRRC,\n",
    "            othello_utils.games_batch_to_state_stack_BLRRC,\n",
    "            othello_utils.games_batch_to_state_stack_mine_yours_BLRRC,\n",
    "        ]\n",
    "    model_name = \"Baidicoot/Othello-GPT-Transformer-Lens\"\n",
    "    # data = construct_othello_dataset(custom_functions, n_inputs, models_path=models_path, device=DEVICE)\n",
    "    indexing_functions = [None]  # I'm experimenting with these for Othello\n",
    "else:\n",
    "    raise ValueError(\"Invalid game\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General dataset statistic\n",
    "\n",
    "This is only dataset dependent, but not SAE dependent and can be calculated once after loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_board_state_counts(pgn_strings):\n",
    "    # Find the true counts of board states over all movers and games in the dataset\n",
    "    # This could be calculated within the board_to_piece_state evaluation!\n",
    "    true_board_states_counts = chess_utils.create_state_stacks(pgn_strings, chess_utils.board_to_piece_state)\n",
    "    true_board_states_counts = chess_utils.state_stack_to_one_hot(\n",
    "        chess_utils.config_lookup[chess_utils.board_to_piece_state.__name__], \n",
    "        DEVICE, \n",
    "        true_board_states_counts)\n",
    "    true_board_states_counts = true_board_states_counts.sum(dim=(0,1))\n",
    "    true_board_states_counts.shape # [RRC]\n",
    "    return true_board_states_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAE specific statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard evals\n",
    "def do_standard_evals(results, ae_bundle):\n",
    "    eval_results = evaluate(\n",
    "        ae_bundle.ae,\n",
    "        ae_bundle.buffer,\n",
    "        max_len=ae_bundle.context_length,\n",
    "        batch_size=min(512, batch_size), # min(n_eval_samples, activation_buffer_out_batch_size) matters\n",
    "        io=\"out\",\n",
    "        device=DEVICE,\n",
    "        n_batches=1000\n",
    "    )\n",
    "    for k, v in eval_results.items():\n",
    "        results[k] = v\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of custom functions\n",
    "def eval_custom_fn(\n",
    "    results,\n",
    "    n_act_threshs,\n",
    "    alive_features_F,\n",
    "    max_activations_F,\n",
    "    ae_bundle,\n",
    "    pgn_strings,\n",
    "    custom_functions,\n",
    "    encoded_inputs,\n",
    "    firing_rate_n_inputs,\n",
    "    indexing_function\n",
    "):\n",
    "    num_features = len(alive_features_F)\n",
    "    print(\n",
    "        f\"Out of {ae_bundle.dictionary_size} features, on {firing_rate_n_inputs} activations, {num_features} are alive.\"\n",
    "    )\n",
    "\n",
    "    assert len(pgn_strings) >= n_inputs\n",
    "    assert n_inputs % batch_size == 0\n",
    "\n",
    "    n_iters = n_inputs // batch_size\n",
    "    # We round up to ensure we don't ignore the remainder of features\n",
    "    num_feature_iters = math.ceil(num_features / feature_batch_size)\n",
    "\n",
    "    thresholds_T = torch.linspace(0, 1, n_act_threshs).to(DEVICE)\n",
    "    thresholds_TF11 = einops.repeat(thresholds_T, \"T -> T F 1 1\", F=num_features)\n",
    "    max_activations_1F11 = einops.repeat(max_activations_F, \"F -> 1 F 1 1\")\n",
    "    thresholds_TF11 = thresholds_TF11 * max_activations_1F11\n",
    "\n",
    "    for i in tqdm(range(n_iters), desc=\"Aggregating statistics\"):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        pgn_strings_BL = pgn_strings[start:end]\n",
    "        encoded_inputs_BL = encoded_inputs[start:end]\n",
    "        encoded_inputs_BL = torch.tensor(encoded_inputs_BL).to(DEVICE)\n",
    "\n",
    "        batch_data = get_data_batch(data, pgn_strings_BL, start, end, custom_functions, DEVICE)\n",
    "\n",
    "        all_activations_FBL, encoded_token_inputs = collect_activations_batch(\n",
    "            ae_bundle, encoded_inputs_BL, alive_features_F\n",
    "        )\n",
    "\n",
    "        if indexing_function is not None:\n",
    "            all_activations_FBL, batch_data = apply_indexing_function(\n",
    "                pgn_strings[start:end], all_activations_FBL, batch_data, DEVICE, indexing_function\n",
    "            )\n",
    "        # For thousands of features, this would be many GB of memory. So, we minibatch.\n",
    "        for feature in range(num_feature_iters):\n",
    "            f_start = feature * feature_batch_size\n",
    "            f_end = min((feature + 1) * feature_batch_size, num_features)\n",
    "            f_batch_size = f_end - f_start\n",
    "\n",
    "            activations_FBL = all_activations_FBL[\n",
    "                f_start:f_end\n",
    "            ]  \n",
    "            \n",
    "            thresholds_TF11_slice = thresholds_TF11[:, f_start:f_end, :, :]\n",
    "            # NOTE: Now F == feature_batch_size\n",
    "            # Maybe that's stupid and inconsistent and I should use a new letter for annotations\n",
    "            # I'll roll with it for now\n",
    "\n",
    "\n",
    "            ### Aggregate batch statistics\n",
    "            active_indices_TFBL = activations_FBL > thresholds_TF11_slice\n",
    "            active_counts_TF = einops.reduce(active_indices_TFBL, \"T F B L -> T F\", \"sum\")\n",
    "            off_counts_TF = einops.reduce(~active_indices_TFBL, \"T F B L -> T F\", \"sum\")\n",
    "\n",
    "            results[\"on_count\"][:, f_start:f_end] += active_counts_TF\n",
    "            results[\"off_count\"][:, f_start:f_end] += off_counts_TF\n",
    "\n",
    "            for custom_function in custom_functions:\n",
    "                on_tracker_TFRRC = results[custom_function.__name__][\"on\"]\n",
    "                off_tracker_FTRRC = results[custom_function.__name__][\"off\"]\n",
    "\n",
    "                boards_BLRRC = batch_data[custom_function.__name__]\n",
    "                boards_TFBLRRC = einops.repeat(\n",
    "                    boards_BLRRC,\n",
    "                    \"B L R1 R2 C -> T F B L R1 R2 C\",\n",
    "                    F=f_batch_size,\n",
    "                    T=thresholds_TF11_slice.shape[0],\n",
    "                )\n",
    "\n",
    "                # TODO The next 2 operations consume almost all of the compute. I don't think it will work,\n",
    "                # but maybe we can only do 1 of these operations?\n",
    "                active_boards_sum_TFRRC = einops.reduce(\n",
    "                    boards_TFBLRRC * active_indices_TFBL[:, :, :, :, None, None, None],\n",
    "                    \"T F B L R1 R2 C -> T F R1 R2 C\",\n",
    "                    \"sum\",\n",
    "                )\n",
    "                off_boards_sum_TFRRC = einops.reduce(\n",
    "                    boards_TFBLRRC * ~active_indices_TFBL[:, :, :, :, None, None, None],\n",
    "                    \"T F B L R1 R2 C -> T F R1 R2 C\",\n",
    "                    \"sum\",\n",
    "                )\n",
    "\n",
    "                on_tracker_TFRRC[:, f_start:f_end, :, :, :] += active_boards_sum_TFRRC\n",
    "                off_tracker_FTRRC[:, f_start:f_end, :, :, :] += off_boards_sum_TFRRC\n",
    "\n",
    "                results[custom_function.__name__][\"on\"] = on_tracker_TFRRC\n",
    "                results[custom_function.__name__][\"off\"] = off_tracker_FTRRC\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, and F1\n",
    "\n",
    "def get_classification_metrics(results, true_board_states_counts):\n",
    "    precision_thresh = 0.9\n",
    "    recall_thresh = 0.01\n",
    "    f1_thresh = 0.01\n",
    "    threshs = [precision_thresh, recall_thresh, f1_thresh]\n",
    "    eps = 1e-8\n",
    "    R = 8\n",
    "    C = 13\n",
    "\n",
    "    true_pos_TFRRC = results['board_to_piece_state']['on'] \n",
    "    pos_all_TF = results['on_count']\n",
    "    true_all_RRC = true_board_states_counts\n",
    "\n",
    "    precision = true_pos_TFRRC / (pos_all_TF[:, :, None, None, None] +eps) # Note that a feature which always fires (piece present/absent) will have a precision of 1\n",
    "    recall = true_pos_TFRRC / (true_all_RRC[None, None, :, :, :] +eps)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + eps)\n",
    "    metrics_TFRRC = [precision, recall, f1]\n",
    "\n",
    "    # Apply threshold\n",
    "    counts_TFRRC = [metric > thresh for metric, thresh in zip(metrics_TFRRC, threshs)]\n",
    "\n",
    "    # Drop empty square state counts\n",
    "    for i in range(len(counts_TFRRC)):\n",
    "        counts_TFRRC[i][..., 6] = False\n",
    "    num_board_states = R * R * (C-1)\n",
    "\n",
    "\n",
    "    ### Fraction of features with high metric on at least one board state\n",
    "    # High metric for at least one board state\n",
    "    counts_any_board_TF = [metric.any(dim=(-1,-2,-3)) for metric in counts_TFRRC]\n",
    "\n",
    "    # Report fraction of all features for count_as_firing_threshold = 0\n",
    "    frac_any_board_nonzero_1 = [metric[0].float().mean() for metric in counts_any_board_TF]\n",
    "\n",
    "    # Report fraction of all features for any threshold (choose threshold per feature that maximizes ratio)\n",
    "    frac_any_board_best_1 = [metric.any(dim=0).float().mean() for metric in counts_any_board_TF]\n",
    "\n",
    "\n",
    "    ### Fraction of board states that have at least one feature with high metric\n",
    "    # Check for each board state whether at least one feature has a high metric (using count_as_firing_threshold = 0)\n",
    "    counts_any_feature_nonzero_RCC = [metric[0].any(dim=0) for metric in counts_TFRRC]\n",
    "\n",
    "    # Check for each board state whether at least one feature has a high metric (for any count_as_firing threshold)\n",
    "    counts_any_feature_best_RCC = [metric.any(dim=(0,1)) for metric in counts_TFRRC]\n",
    "\n",
    "    # Fraction of individual board states at least one feature has a high metric\n",
    "    frac_any_feature_nonzero_RCC = [metric.sum() / num_board_states for metric in counts_any_feature_nonzero_RCC]\n",
    "    frac_any_feature_best_RCC = [metric.sum() / num_board_states for metric in counts_any_feature_best_RCC]\n",
    "\n",
    "    print(frac_any_board_nonzero_1)\n",
    "    print(frac_any_board_best_1)\n",
    "    print(frac_any_feature_nonzero_RCC)\n",
    "    print(frac_any_feature_best_RCC)\n",
    "\n",
    "    names = ['precision', 'recall', 'f1']\n",
    "    for i, (name, t) in enumerate(zip(names, threshs)):\n",
    "        results[f'frac_any_board_per_feature_act-nonzero_{name}-{t}'] = frac_any_board_nonzero_1[i].item()\n",
    "        results[f'frac_any_board_per_feature_act-best_{name}-{t}'] = frac_any_board_best_1[i].item()\n",
    "        results[f'frac_any_feature_per_board_act-nonzero_{name}-{t}'] = frac_any_feature_nonzero_RCC[i].item()\n",
    "        results[f'frac_any_feature_per_board_act-best_{name}-{t}'] = frac_any_feature_best_RCC[i].item()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose aes and indexing functions\n",
    "\n",
    "# This could be computed once before the loop if adapting loading pgn_strings\n",
    "# true_board_state_counts = get_true_board_state_counts(pgn_strings)\n",
    "\n",
    "sweep_results = {}\n",
    "sweep_result_keys = ['l0', 'frac_variance_explained', 'cossim', 'l2_ratio', 'frac_any_board_per_feature_act-nonzero_precision-0.9', 'frac_any_board_per_feature_act-best_precision-0.9', 'frac_any_feature_per_board_act-nonzero_precision-0.9', 'frac_any_feature_per_board_act-best_precision-0.9', 'frac_any_board_per_feature_act-nonzero_recall-0.01', 'frac_any_board_per_feature_act-best_recall-0.01', 'frac_any_feature_per_board_act-nonzero_recall-0.01', 'frac_any_feature_per_board_act-best_recall-0.01', 'frac_any_board_per_feature_act-nonzero_f1-0.01', 'frac_any_board_per_feature_act-best_f1-0.01', 'frac_any_feature_per_board_act-nonzero_f1-0.01', 'frac_any_feature_per_board_act-best_f1-0.01']\n",
    "\n",
    "all_autoencoder_paths = []\n",
    "for group_path in autoencoder_group_paths:\n",
    "    all_autoencoder_paths += get_nested_folders(repo_dir + group_path) \n",
    "\n",
    "param_combinations = list(itertools.product(all_autoencoder_paths, indexing_functions))\n",
    "\n",
    "for ae_dir, idx_fn in param_combinations:\n",
    "    print(f'ae_dir: {ae_dir}')\n",
    "    print(f'idx_fn: {idx_fn}\\n')\n",
    "\n",
    "# autoencoder_path, indexing_function = param_combinations[1]\n",
    "\n",
    "for autoencoder_path, indexing_function in tqdm(param_combinations, desc=\"Autoencoder loop\", total=len(param_combinations)):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    indexing_function_name = \"None\"\n",
    "    if indexing_function is not None:\n",
    "        indexing_function_name = indexing_function.__name__\n",
    "\n",
    "    print(f\"Autoencoder: {autoencoder_path}\")\n",
    "    print(f\"Indexing function: {indexing_function_name}\")\n",
    "\n",
    "    # TODO Function below manipulates the loaded data. If we change that, we can load data once and for all at the top of the file\n",
    "    data = construct_eval_dataset(custom_functions, n_inputs, models_path=models_path, device=DEVICE)\n",
    "    data, ae_bundle, pgn_strings, encoded_inputs = prep_firing_rate_data(\n",
    "        autoencoder_path, batch_size, models_path, model_name, data, DEVICE, n_inputs, othello\n",
    "    )\n",
    "\n",
    "    firing_rate_n_inputs = min(int(n_inputs * 0.5), 1000) * ae_bundle.context_length\n",
    "    # TODO: Custom thresholds per feature based on max activations\n",
    "    alive_features_F, max_activations_F = get_firing_features(\n",
    "        ae_bundle, firing_rate_n_inputs, batch_size, DEVICE\n",
    "    )\n",
    "    true_board_states_counts = get_true_board_state_counts(pgn_strings)\n",
    "    assert true_board_states_counts is not None\n",
    "\n",
    "    # initialize result dictionary\n",
    "    n_act_threshs = 10\n",
    "    results = initialize_results_dict(custom_functions, n_act_threshs, alive_features_F, DEVICE)\n",
    "\n",
    "    # Standard evaluation metrics\n",
    "    print('do_standard_evals')\n",
    "    results = do_standard_evals(results, ae_bundle)\n",
    "    del ae_bundle.buffer\n",
    "    \n",
    "    # Do custom eval metrics\n",
    "    print('do custom eval metrics')\n",
    "    results = eval_custom_fn(\n",
    "        results,\n",
    "        n_act_threshs,\n",
    "        alive_features_F,\n",
    "        max_activations_F,\n",
    "        ae_bundle,\n",
    "        pgn_strings,\n",
    "        custom_functions,\n",
    "        encoded_inputs,\n",
    "        firing_rate_n_inputs,\n",
    "        indexing_function,\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    results = get_classification_metrics(results, true_board_states_counts)\n",
    "    ae_name = autoencoder_path.split('/')[-2]\n",
    "    sweep_results[ae_name] = {}\n",
    "    for sweep_key in sweep_result_keys:\n",
    "        sweep_results[ae_name][sweep_key] = results[sweep_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(sweep_results, orient='index').sort_values('l0')\n",
    "df = df.round(3)\n",
    "df[['frac_variance_explained', 'l0', 'frac_any_board_per_feature_act-nonzero_precision-0.9', 'frac_any_board_per_feature_act-best_precision-0.9', 'frac_any_feature_per_board_act-nonzero_precision-0.9', 'frac_any_feature_per_board_act-best_precision-0.9', 'frac_any_board_per_feature_act-nonzero_recall-0.01', 'frac_any_board_per_feature_act-best_recall-0.01', 'frac_any_feature_per_board_act-nonzero_recall-0.01', 'frac_any_feature_per_board_act-best_recall-0.01', 'frac_any_board_per_feature_act-nonzero_f1-0.01', 'frac_any_board_per_feature_act-best_f1-0.01', 'frac_any_feature_per_board_act-nonzero_f1-0.01', 'frac_any_feature_per_board_act-best_f1-0.01']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
