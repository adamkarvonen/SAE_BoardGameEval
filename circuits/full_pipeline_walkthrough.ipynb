{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import os\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n",
    "import circuits.analysis as analysis\n",
    "import circuits.eval_board_reconstruction as eval_board_reconstruction\n",
    "import circuits.get_eval_results as get_eval_results\n",
    "import circuits.f1_analysis as f1_analysis\n",
    "import circuits.utils as utils\n",
    "import circuits.pipeline_config as pipeline_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some optional parameters you can change, but it will run without issue using the defaults.\n",
    "\n",
    "We just need to pass in `autoencoder_path` and `autoencoder_group_path` and it will load all of the required information.\n",
    "\n",
    "At a batch size of 5 and `config.analysis_on_cpu`, peak GPU memory usage is around 2.5 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "autoencoder_group_path = \"../autoencoders/testing_chess/\"\n",
    "autoencoder_path = \"../autoencoders/testing_chess/trainer4/\"\n",
    "\n",
    "othello = eval_sae.check_if_autoencoder_is_othello(autoencoder_group_path)\n",
    "config = pipeline_config.Config()\n",
    "\n",
    "# These both significantly reduce peak GPU memory usage\n",
    "config.batch_size = 5\n",
    "config.analysis_on_cpu = True\n",
    "\n",
    "# Precompute will create both datasets and save them as pickle files\n",
    "# If precompute == False, it creates the dataset on the fly\n",
    "# This is far slower when evaluating multiple SAEs, but for an exploratory run it is fine\n",
    "config.precompute = False\n",
    "\n",
    "config.eval_results_n_inputs = 500\n",
    "config.eval_sae_n_inputs = 500\n",
    "config.board_reconstruction_n_inputs = 500\n",
    "\n",
    "# Once you have ran the analysis, you can set this to False and it will load the saved results\n",
    "config.run_analysis = True\n",
    "config.run_board_reconstruction = True\n",
    "config.run_eval_sae = False\n",
    "config.run_eval_results = True\n",
    "\n",
    "# If you want to save the results of the analysis\n",
    "config.save_results = True\n",
    "config.save_feature_labels = True\n",
    "\n",
    "print(f\"Is Othello: {othello}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create separate train and test datasets. By default, we don't precompute the board states, so the dictionaries will just contain encoded and decoded input strings. For chess, the decoded input strings are PGN strings (1.e4 e5 2.Nf3 ...) and the encoded strings are a list of integers, where every integer corresponds to a character.\n",
    "\n",
    "The board states will be computed on the fly if `precompute == False`. The tensor for `board_to_piece_state` will be of shape (batch size, seq length, rows, columns, classes) or (batch size, 20, 8, 8, 13).\n",
    "\n",
    "20: There are 20 periods (which means white's turn to move) in a PGN string of 256 characters.\n",
    "\n",
    "8: rows / columns\n",
    "\n",
    "13: Total number of piece types (black king, black queen, blank, ... white king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = max(config.eval_sae_n_inputs, config.board_reconstruction_n_inputs)\n",
    "\n",
    "# We have plenty of data and eval_results_data doesn't use VRAM, so we can afford to make it large\n",
    "# So we don't hit the end of the activation buffer\n",
    "eval_results_dataset_size = config.eval_results_n_inputs * 10\n",
    "\n",
    "indexing_functions = eval_sae.get_recommended_indexing_functions(othello)\n",
    "indexing_function = indexing_functions[0]\n",
    "\n",
    "if othello:\n",
    "    custom_functions = config.othello_functions\n",
    "    game_name = \"othello\"\n",
    "else:\n",
    "    custom_functions = config.chess_functions\n",
    "    game_name = \"chess\"\n",
    "\n",
    "train_dataset_name = f\"{game_name}_train_dataset.pkl\"\n",
    "test_dataset_name = f\"{game_name}_test_dataset.pkl\"\n",
    "\n",
    "if os.path.exists(train_dataset_name) and config.precompute:\n",
    "    print(\"Loading statistics aggregation dataset\")\n",
    "    with open(train_dataset_name, \"rb\") as f:\n",
    "        train_data = pickle.load(f)\n",
    "else:\n",
    "    print(\"Constructing statistics aggregation dataset\")\n",
    "    train_data = eval_sae.construct_dataset(\n",
    "        othello,\n",
    "        custom_functions,\n",
    "        dataset_size,\n",
    "        split=\"train\",\n",
    "        device=device,\n",
    "        precompute_dataset=config.precompute,\n",
    "    )\n",
    "    if config.precompute:\n",
    "        print(\"Saving statistics aggregation dataset\")\n",
    "        with open(train_dataset_name, \"wb\") as f:\n",
    "            pickle.dump(train_data, f)\n",
    "\n",
    "if os.path.exists(test_dataset_name) and config.precompute:\n",
    "    print(\"Loading test dataset\")\n",
    "    with open(test_dataset_name, \"rb\") as f:\n",
    "        test_data = pickle.load(f)\n",
    "else:\n",
    "    print(\"Constructing test dataset\")\n",
    "    test_data = eval_sae.construct_dataset(\n",
    "        othello,\n",
    "        custom_functions,\n",
    "        dataset_size,\n",
    "        split=\"test\",\n",
    "        device=device,\n",
    "        precompute_dataset=config.precompute,\n",
    "    )\n",
    "    if config.precompute:\n",
    "        print(\"Saving test dataset\")\n",
    "        with open(test_dataset_name, \"wb\") as f:\n",
    "            pickle.dump(test_data, f)\n",
    "\n",
    "eval_results_data = eval_sae.construct_dataset(\n",
    "    othello,\n",
    "    [],\n",
    "    eval_results_dataset_size,\n",
    "    split=\"train\",\n",
    "    device=device,\n",
    "    precompute_dataset=config.precompute,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run an evaluation to get some standard sparse autoencoder metrics, such as L0 and loss recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_eval_results_output_location = get_eval_results.get_output_location(\n",
    "    autoencoder_path, n_inputs=config.eval_results_n_inputs\n",
    ")\n",
    "\n",
    "if config.run_eval_results:\n",
    "\n",
    "    # If this is set, everything below should be reproducible\n",
    "    # Then we can just save results from 1 run, make optimizations, and check that the results are the same\n",
    "    # The determinism is only needed for getting activations from the activation buffer for finding alive features\n",
    "    torch.manual_seed(0)\n",
    "    eval_results = get_eval_results.get_evals(\n",
    "        autoencoder_path,\n",
    "        config.eval_results_n_inputs,\n",
    "        config.batch_size,\n",
    "        device,\n",
    "        utils.to_device(eval_results_data.copy(), device),\n",
    "        othello=othello,\n",
    "        save_results=config.save_results,\n",
    "    )\n",
    "else:\n",
    "    with open(expected_eval_results_output_location, \"rb\") as f:\n",
    "        eval_results = pickle.load(f)\n",
    "    eval_results = utils.to_device(eval_results, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_results[\"eval_results\"].keys())\n",
    "print(f\"L0: {eval_results['eval_results']['l0']}\")\n",
    "print(f\"Loss recovered: {eval_results['eval_results']['frac_recovered']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the statistics aggregation, or the \"training\" phase. This will take a couple minutes to run depending on GPU. I will explain what this does in future cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_aggregation_output_location = eval_sae.get_output_location(\n",
    "    autoencoder_path,\n",
    "    n_inputs=config.eval_sae_n_inputs,\n",
    "    indexing_function=indexing_function,\n",
    ")\n",
    "\n",
    "if config.run_eval_sae:\n",
    "    print(\"Aggregating\", autoencoder_path)\n",
    "    aggregation_results = eval_sae.aggregate_statistics(\n",
    "        custom_functions=custom_functions,\n",
    "        autoencoder_path=autoencoder_path,\n",
    "        n_inputs=config.eval_sae_n_inputs,\n",
    "        batch_size=config.batch_size,\n",
    "        device=device,\n",
    "        data=utils.to_device(train_data.copy(), device),\n",
    "        thresholds_T=config.f1_analysis_thresholds,\n",
    "        indexing_function=indexing_function,\n",
    "        othello=othello,\n",
    "        save_results=config.save_results,\n",
    "        precomputed=config.precompute,\n",
    "    )\n",
    "else:\n",
    "    with open(expected_aggregation_output_location, \"rb\") as f:\n",
    "        aggregation_results = pickle.load(f)\n",
    "    aggregation_results = utils.to_device(aggregation_results, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the `aggregation_results` and use them to calculate the `feature_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.analysis_on_cpu:\n",
    "    aggregation_results = utils.to_device(aggregation_results, \"cpu\")\n",
    "    analysis_device = \"cpu\"\n",
    "else:\n",
    "    analysis_device = device\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "expected_feature_labels_output_location = expected_aggregation_output_location.replace(\n",
    "    \"results.pkl\", \"feature_labels.pkl\"\n",
    ")\n",
    "if config.run_analysis:\n",
    "    feature_labels, misc_stats = analysis.analyze_results_dict(\n",
    "        aggregation_results,\n",
    "        output_path=expected_feature_labels_output_location,\n",
    "        device=analysis_device,\n",
    "        high_threshold=config.analysis_high_threshold,\n",
    "        low_threshold=config.analysis_low_threshold,\n",
    "        significance_threshold=config.analysis_significance_threshold,\n",
    "        verbose=False,\n",
    "        print_results=False,\n",
    "        save_results=config.save_feature_labels,\n",
    "    )\n",
    "else:\n",
    "    with open(expected_feature_labels_output_location, \"rb\") as f:\n",
    "        feature_labels = pickle.load(f)\n",
    "    feature_labels = utils.to_device(feature_labels, analysis_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting / display functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rc_to_square_notation(row, col):\n",
    "    letters = \"ABCDEFGH\"\n",
    "    number = row + 1\n",
    "    letter = letters[col]\n",
    "    return f\"{letter}{number}\"\n",
    "\n",
    "def plot_board(board_RR: torch.Tensor, title: str = \"Board\", png_filename: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Plots an 8x8 board with the value of the maximum square displayed in red text to two decimal places.\n",
    "\n",
    "    Args:\n",
    "        board_RR (torch.Tensor): A 2D tensor of shape (8, 8) with values from 0 to 1.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    assert board_RR.shape == (8, 8), \"board_RR must be of shape 8x8\"\n",
    "\n",
    "    # Flip the board vertically\n",
    "    board_RR = torch.flip(board_RR, [0])\n",
    "\n",
    "    plt.imshow(board_RR, cmap='gray_r', interpolation='none', vmin=0, vmax=1)\n",
    "    plt.colorbar()  # Adds a colorbar to help identify the values\n",
    "    plt.title(title)\n",
    "\n",
    "    # Set labels for columns (A-H)\n",
    "    plt.xticks(range(8), ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'])\n",
    "\n",
    "    # Set labels for rows (1-8)\n",
    "    plt.yticks(range(8), range(1, 9))\n",
    "\n",
    "    # Add gridlines mimicking a chess board\n",
    "    # plt.grid(True, color='black', linewidth=1, linestyle='-', alpha=0.5)\n",
    "    # plt.tick_params(bottom=False, left=False, labelbottom=True, labelleft=True)\n",
    "\n",
    "    # Offset gridlines by 0.5 in x and y\n",
    "    plt.gca().set_xticks([x - 0.5 for x in range(1, 9)], minor=True)\n",
    "    plt.gca().set_yticks([y - 0.5 for y in range(1, 9)], minor=True)\n",
    "    plt.grid(True, which='minor', color='black', linewidth=1, linestyle='-', alpha=0.5)\n",
    "\n",
    "    # Find the maximum value and its position\n",
    "    max_value, max_pos = torch.max(board_RR), torch.argmax(board_RR)\n",
    "    max_i, max_j = torch.div(max_pos, 8, rounding_mode='floor'), max_pos % 8\n",
    "\n",
    "    # Display the maximum value in red text at the corresponding position\n",
    "    plt.text(max_j, max_i, f\"{max_value:.2f}\", color='red', ha='center', va='center', fontsize=12)\n",
    "\n",
    "    if png_filename is not None:\n",
    "        plt.savefig(png_filename)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "num_to_class = {0: \"Black King\", 1: \"Black Queen\", 2: \"Black Rook\", 3: \"Black Bishop\", 4: \"Black Knight\", 5: \"Black Pawn\",\n",
    "                6: \"Blank\", 7: \"White Pawn\", 8: \"White Knight\", 9: \"White Bishop\", 10: \"White Rook\", 11: \"White Queen\", 12: \"White King\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `feature_labels`? It's a dict of board to state function: tensor. Various board to state functions include `board_to_pin_state`, `board_to_piece_masked_blank_and_initial_state`, etc.\n",
    "\n",
    "The feature labels tensor is of shape (num_thresholds, num_alive_features, rows, columns, classes).\n",
    "\n",
    "If feature labels `board_to_piece_masked_blank_and_initial_state` has a value of 1 at `threshold==5`, `feature_idx==173`, `row==2`, `column==2`, `classes==7`, then that means when feature 173 is active over a threshold of 50%, there is over a 95% chance there is a white pawn of C3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_of_interest = \"board_to_piece_masked_blank_and_initial_state\"\n",
    "\n",
    "board_state_feature_labels_TFRRC = feature_labels[function_of_interest]\n",
    "print(f\"Board state feature labels: {board_state_feature_labels_TFRRC.shape}\")\n",
    "threshold = 2\n",
    "\n",
    "board_state_feature_labels_FRRC = board_state_feature_labels_TFRRC[threshold]\n",
    "board_state_counts_F = einops.reduce(board_state_feature_labels_FRRC, \"F R1 R2 C -> F\", \"sum\")\n",
    "\n",
    "max_features = 175\n",
    "demo_idx = 0\n",
    "for i in range(max_features):\n",
    "    if board_state_counts_F[i] > 0:\n",
    "        print(f\"Feature {i} has {board_state_counts_F[i]} classified squares\")\n",
    "        demo_idx = i\n",
    "\n",
    "demo_feature_labels_RRC = board_state_feature_labels_FRRC[demo_idx]\n",
    "print(f\"\\nFeature {demo_idx} has {board_state_counts_F[demo_idx].sum().item()} classified squares\")\n",
    "\n",
    "classified_squares = torch.where(demo_feature_labels_RRC == 1)\n",
    "print(f\"Classified squares as tensors: {classified_squares}\")\n",
    "\n",
    "row, column, classes = classified_squares\n",
    "\n",
    "print(f\"\\nClassified squares for feature {demo_idx} at threshold {threshold}:\")\n",
    "for i in range(row.shape[0]):\n",
    "    print(rc_to_square_notation(row[i].item(), column[i].item()), num_to_class[classes[i].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How were these feature labels computed? We start with `aggregation_results`. It contains the average state of the board for every board state function for every feature when it is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aggregation_results['on_count'].shape)\n",
    "\n",
    "T, F = aggregation_results[\"on_count\"].shape\n",
    "\n",
    "print(f\"For all {F} alive features over {T} thresholds, on_count is the number of times each feature is on above every threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the feature label was determined for this square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row = row[0].item()\n",
    "example_column = column[0].item()\n",
    "example_class = classes[0].item()\n",
    "\n",
    "example_on_count = aggregation_results[\"on_count\"][threshold, demo_idx]\n",
    "example_present_count = aggregation_results[function_of_interest]['on'][threshold, demo_idx, example_row, example_column, example_class].item()\n",
    "\n",
    "print(f\"Feature {demo_idx} was active {example_on_count} times above threshold {threshold}\")\n",
    "print(f\"During these activations, there was a {num_to_class[example_class]} at {rc_to_square_notation(example_row, example_column)}\")\n",
    "print(f\"{example_present_count} times, or {example_present_count / example_on_count:.0%} of the time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_results = analysis.add_off_tracker(aggregation_results, custom_functions, analysis_device)\n",
    "\n",
    "formatted_results = analysis.normalize_tracker(\n",
    "    formatted_results,\n",
    "    \"on\",\n",
    "    custom_functions,\n",
    "    analysis_device,\n",
    ")\n",
    "\n",
    "formatted_results = analysis.normalize_tracker(\n",
    "    formatted_results,\n",
    "    \"off\",\n",
    "    custom_functions,\n",
    "    analysis_device,\n",
    ")\n",
    "\n",
    "print(formatted_results[\"board_to_piece_masked_blank_and_initial_state\"]['on_normalized'].shape)\n",
    "\n",
    "board_results_TFRRC = formatted_results[\"board_to_piece_masked_blank_and_initial_state\"]['on_normalized']\n",
    "\n",
    "def plot_feature_board_states(board_results_TFRRC: torch.Tensor, feature_idx: int, threshold: int, piece_type: int):\n",
    "    results_RRC = board_results_TFRRC[threshold, feature_idx]\n",
    "\n",
    "    feature_on_count = formatted_results['on_count'][threshold, feature_idx]\n",
    "\n",
    "    print(f\"Feature {feature_idx} had {int(feature_on_count)} activations over threshold {(threshold * 10)}%\")\n",
    "\n",
    "    print(results_RRC.shape)\n",
    "    results_RR = results_RRC[..., piece_type]\n",
    "    print(results_RR)\n",
    "\n",
    "    title = f\"Average {num_to_class[piece_type]} activation for \\nfeature {feature_idx} over threshold {(threshold * 10)}%\"\n",
    "    png_filename = f\"feature_{feature_idx}_threshold_{threshold}_piece_{piece_type}.png\"\n",
    "    plot_board(results_RR, title, png_filename)\n",
    "\n",
    "# plot_feature_board_states(board_results_TFRRC, demo_idx, 0, 8)\n",
    "# plot_feature_board_states(board_results_TFRRC, demo_idx, 2, 8)\n",
    "# plot_feature_board_states(board_results_TFRRC, demo_idx, 2, 5)\n",
    "# plot_feature_board_states(board_results_TFRRC, demo_idx, 0, 3)\n",
    "# plot_feature_board_states(board_results_TFRRC, demo_idx, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we increase the threshold, the precision of the prediction increases\n",
    "\n",
    "plot_feature_board_states(board_results_TFRRC, demo_idx, 0, 8)\n",
    "plot_feature_board_states(board_results_TFRRC, demo_idx, 2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes without a feature label don't have high precision predictions\n",
    "\n",
    "plot_feature_board_states(board_results_TFRRC, demo_idx, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the threshold increases, a square that was not a feature label may become one as the precision increases\n",
    "\n",
    "plot_feature_board_states(board_results_TFRRC, demo_idx, 0, 3)\n",
    "plot_feature_board_states(board_results_TFRRC, demo_idx, 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move everything back to our device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.analysis_on_cpu:\n",
    "    aggregation_results = utils.to_device(aggregation_results, device)\n",
    "    feature_labels = utils.to_device(feature_labels, device)\n",
    "    misc_stats = utils.to_device(misc_stats, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use these feature labels to reconstruct the state of the board as measured by all board state functions. At every board state, we reconstruct it using only SAE feature activations and `feature_labels`. We measure the accuracy of the reconstructed board using F1 score. This will take a few minutes to run depending on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, this can be sped up by\n",
    "# config.board_reconstruction_n_inputs = 100\n",
    "\n",
    "expected_reconstruction_output_location = expected_aggregation_output_location.replace(\n",
    "    \"results.pkl\", \"reconstruction.pkl\"\n",
    ")\n",
    "\n",
    "if config.run_board_reconstruction:\n",
    "    print(\"Testing board reconstruction\")\n",
    "    board_reconstruction_results = eval_board_reconstruction.test_board_reconstructions(\n",
    "        custom_functions=custom_functions,\n",
    "        autoencoder_path=autoencoder_path,\n",
    "        feature_labels=feature_labels,\n",
    "        output_file=expected_reconstruction_output_location,\n",
    "        n_inputs=config.board_reconstruction_n_inputs,\n",
    "        batch_size=config.batch_size,\n",
    "        device=device,\n",
    "        data=utils.to_device(test_data.copy(), device),\n",
    "        othello=othello,\n",
    "        print_results=False,\n",
    "        save_results=config.save_results,\n",
    "        precomputed=config.precompute,\n",
    "    )\n",
    "else:\n",
    "    with open(expected_reconstruction_output_location, \"rb\") as f:\n",
    "        board_reconstruction_results = pickle.load(f)\n",
    "    board_reconstruction_results = utils.to_device(board_reconstruction_results, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then view (F1 score, number of true positives, false positives, false negatives, etc) per threshold for every function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_of_interest = \"board_to_piece_masked_blank_and_initial_state\"\n",
    "\n",
    "print(board_reconstruction_results.keys())\n",
    "print(board_reconstruction_results[function_of_interest].keys())\n",
    "print(board_reconstruction_results[function_of_interest]['f1_score_per_class'])\n",
    "\n",
    "threshold = 2\n",
    "\n",
    "print(f\"At threshold {threshold}, this SAE reconstructed {function_of_interest} with an F1 score of {board_reconstruction_results[function_of_interest]['f1_score_per_class'][threshold]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
