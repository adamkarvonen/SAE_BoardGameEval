{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import einops\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from matplotlib.colors import Normalize\n",
    "from typing import Callable\n",
    "import json\n",
    "\n",
    "import circuits.analysis as analysis\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n",
    "import circuits.chess_utils as chess_utils\n",
    "import circuits.utils as utils\n",
    "import circuits.f1_analysis as f1_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been trying to balance: Having a notebook that's easy to play around with, but also modularizing code into files, and not having duplicated functions between notebooks and python files that goes stale.\n",
    "\n",
    "Here's what I'm trying: All of the following functions in the next few cells are also in `f1_analysis.py`. By default, this notebook just loads a `f1_results.csv` from the autoencoder group path, automatically generated during `full_pipeline.ipynb`. If you want to play around with new analysis functions, feel free to uncomment the lines that call these functions, modify the below functions, and experiment to your heart's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_f1_average(f1_TFRRC: torch.Tensor, config: chess_utils.Config) -> torch.Tensor:\n",
    "    \"\"\"For every threshold, for every square, find the best F1 score across all features. Then average across all squares.\n",
    "    NOTE: If the function is binary, num_squares == 1. If it is board to piece state, num_squares == 8 * 8 * 12\"\"\"\n",
    "    f1_TRRC, _ = torch.max(f1_TFRRC, dim=1)\n",
    "\n",
    "    T, R1, R2, C = f1_TRRC.shape\n",
    "\n",
    "    if config.one_hot_mask_idx is not None:\n",
    "        C -= 1\n",
    "\n",
    "    max_possible = R1 * R2 * C\n",
    "\n",
    "    f1_T = einops.reduce(f1_TRRC, 'T R1 R2 C -> T', 'sum') / max_possible\n",
    "\n",
    "    return f1_T\n",
    "    \n",
    "\n",
    "def f1s_above_threshold(f1_TFRRC: torch.Tensor, threshold: float, config: chess_utils.Config) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"For every threshold, for every square, find the best F1 score across all features. Then, find the number of squares that have a F1 score above the threshold.\n",
    "    If the function is binary, num_squares == 1. If it is board to piece state, num_squares == 8 * 8 * 12\n",
    "    NOTE: This will probably be most useful for features with 8x8xn options.\"\"\"\n",
    "    f1_TRRC, _ = torch.max(f1_TFRRC, dim=1)\n",
    "\n",
    "    f1s_above_threshold_TRCC = f1_TRRC > threshold\n",
    "\n",
    "    T, R1, R2, C = f1_TRRC.shape\n",
    "    if config.one_hot_mask_idx is not None:\n",
    "        C -= 1\n",
    "\n",
    "    max_possible = R1 * R2 * C\n",
    "\n",
    "    f1_T = einops.reduce(f1s_above_threshold_TRCC, 'T R1 R2 C -> T', 'sum')\n",
    "\n",
    "    f1_T_normalized = f1_T / max_possible\n",
    "\n",
    "    return f1_T, f1_T_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(analysis)\n",
    "\n",
    "\n",
    "def get_custom_functions(\n",
    "    autoencoder_group_path: str, results_filename_filter: str, device: str\n",
    ") -> list[Callable]:\n",
    "    folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "    first_autoencoder_path = folders[0]\n",
    "    results_filenames = analysis.get_all_results_file_names(\n",
    "        first_autoencoder_path, results_filename_filter\n",
    "    )\n",
    "\n",
    "    if len(results_filenames) > 1 or len(results_filenames) == 0:\n",
    "        raise ValueError(\"There are multiple results files\")\n",
    "    results_filename = results_filenames[0]\n",
    "\n",
    "    with open(first_autoencoder_path + results_filename, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    results = utils.to_device(results, device)\n",
    "\n",
    "    custom_functions = analysis.get_all_custom_functions(results)\n",
    "    return custom_functions\n",
    "\n",
    "\n",
    "def get_custom_function_names(custom_functions: list[Callable]) -> list[str]:\n",
    "    custom_function_names = [function.__name__ for function in custom_functions]\n",
    "    return custom_function_names\n",
    "\n",
    "def get_threshold_column_names(func_name: str, threshold: float) -> tuple[str, str]:\n",
    "    return (f\"{func_name}_f1_threshold_{threshold}\", f\"{func_name}_f1_threshold_{threshold}_normalized\")\n",
    "\n",
    "\n",
    "def get_all_sae_f1_results(\n",
    "    autoencoder_group_paths: list[str],\n",
    "    df: pd.DataFrame,\n",
    "    results_filename_filter: str,\n",
    "    custom_functions: list[Callable],\n",
    "    custom_function_names: list[str],\n",
    "    device: str,\n",
    "    thresholds: list[float],\n",
    "    mask: bool,\n",
    ") -> dict:\n",
    "    all_sae_results = {}\n",
    "\n",
    "    for autoencoder_group_path in autoencoder_group_paths:\n",
    "\n",
    "        folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "        sae_results = {}\n",
    "\n",
    "        for autoencoder_path in folders:\n",
    "\n",
    "            print(f\"Processing {autoencoder_path}\")\n",
    "\n",
    "            assert (\n",
    "                autoencoder_path in df[\"autoencoder_path\"].values\n",
    "            ), f\"{autoencoder_path} not in csv file\"\n",
    "\n",
    "            sae_results[autoencoder_path] = {}\n",
    "\n",
    "            results_filenames = analysis.get_all_results_file_names(\n",
    "                autoencoder_path, results_filename_filter\n",
    "            )\n",
    "            if len(results_filenames) > 1 or len(results_filenames) == 0:\n",
    "                print(\n",
    "                    f\"Skipping {autoencoder_path} because it has {len(results_filenames)} results files\"\n",
    "                )\n",
    "                print(\"This is most likely because there are results files from different n_inputs\")\n",
    "                continue\n",
    "            results_filename = results_filenames[0]\n",
    "\n",
    "            with open(autoencoder_path + results_filename, \"rb\") as f:\n",
    "                results = pickle.load(f)\n",
    "\n",
    "            results = utils.to_device(results, device)\n",
    "\n",
    "            results = analysis.add_off_tracker(results, custom_functions, device)\n",
    "            f1_dict_TFRRC = analysis.get_all_f1s(results, device)\n",
    "\n",
    "            # feature_labels = analysis.analyze_results_dict(\n",
    "            #     results,\n",
    "            #     output_path=\"\",\n",
    "            #     device=device,\n",
    "            #     high_threshold=0.95,\n",
    "            #     low_threshold=0.1,\n",
    "            #     significance_threshold=10,\n",
    "            #     save_results=False,\n",
    "            #     mask=mask,\n",
    "            #     verbose=False,\n",
    "            #     print_results=False,\n",
    "            # )\n",
    "\n",
    "            correct_row = df[\"autoencoder_path\"] == autoencoder_path\n",
    "            sae_results[autoencoder_path][\"l0\"] = df[correct_row][\"l0\"].values[0]\n",
    "            sae_results[autoencoder_path][\"frac_variance_explained\"] = df[correct_row][\n",
    "                \"frac_variance_explained\"\n",
    "            ].values[0]\n",
    "\n",
    "            for func_name in f1_dict_TFRRC:\n",
    "                config = chess_utils.config_lookup[func_name]\n",
    "                custom_function = config.custom_board_state_function\n",
    "                assert (\n",
    "                    custom_function in custom_functions\n",
    "                ), f\"Key {custom_function} not in custom_functions\"\n",
    "                f1_TFRRC = f1_dict_TFRRC[func_name]\n",
    "\n",
    "                average_f1_T = best_f1_average(f1_TFRRC, config)\n",
    "                sae_results[autoencoder_path][f\"{func_name}_average_f1\"] = average_f1_T\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    threshold_column, threshold_column_normalized = get_threshold_column_names(\n",
    "                        func_name, threshold\n",
    "                    )\n",
    "                    f1_T, f1_T_normalized = f1s_above_threshold(f1_TFRRC, threshold, config)\n",
    "                    sae_results[autoencoder_path][threshold_column] = f1_T\n",
    "                    sae_results[autoencoder_path][threshold_column_normalized] = f1_T_normalized\n",
    "\n",
    "            # torch.cuda.empty_cache()\n",
    "        all_sae_results[autoencoder_group_path] = sae_results\n",
    "    return all_sae_results\n",
    "\n",
    "def update_dataframe_with_results(df, all_sae_results, custom_function_names, autoencoder_group_paths, thresholds):\n",
    "    assert df['autoencoder_path'].is_unique\n",
    "    updates = []\n",
    "    for autoencoder_group_path in autoencoder_group_paths:\n",
    "        folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "        for autoencoder_path in folders:\n",
    "            results = {'autoencoder_path': autoencoder_path}\n",
    "            for func_name in custom_function_names:\n",
    "        \n",
    "                f1_T = all_sae_results[autoencoder_group_path][autoencoder_path][f\"{func_name}_average_f1\"]\n",
    "                best_idx = torch.argmax(f1_T)\n",
    "                best_f1 = f1_T[best_idx]\n",
    "                \n",
    "                results[f\"{func_name}_best_average_f1\"] = best_f1.item()\n",
    "                results[f\"{func_name}_best_average_f1_idx\"] = best_idx.item()\n",
    "                results[f\"{func_name}_all_average_f1s\"] = json.dumps(f1_T.tolist())\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    threshold_column, threshold_normalized_column = get_threshold_column_names(func_name, threshold)\n",
    "                    f1_T = all_sae_results[autoencoder_group_path][autoencoder_path][threshold_column]\n",
    "                    f1_T_normalized = all_sae_results[autoencoder_group_path][autoencoder_path][threshold_normalized_column]\n",
    "                    best_idx = torch.argmax(f1_T)\n",
    "                    best_f1_at_threshold = f1_T[best_idx]\n",
    "                    best_f1_normalized = f1_T_normalized[best_idx]\n",
    "                    \n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_best\"] = best_f1_at_threshold.item()\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_best_normalized\"] = best_f1_normalized.item()\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_best_idx\"] = best_idx.item()\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_best_normalized_idx\"] = best_idx.item()\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_all\"] = json.dumps(f1_T.tolist())\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_all_normalized\"] = json.dumps(f1_T_normalized.tolist())\n",
    "\n",
    "            \n",
    "            updates.append(results)\n",
    "\n",
    "    update_df = pd.DataFrame(updates)\n",
    "    df = pd.merge(df, update_df, on='autoencoder_path', how='outer')\n",
    "    assert df['autoencoder_path'].is_unique\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set autoencoder group paths and thresholds here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda\"\n",
    "# mask = False\n",
    "# thresholds = torch.arange(0.1, 1, 0.1)\n",
    "\n",
    "# autoencoder_group_paths = [\"../autoencoders/chess_layer5_large_sweep/\"]\n",
    "# autoencoder_group_paths = [\"../autoencoders/group-2024-05-14_chess/\"]\n",
    "# autoencoder_group_paths = [\"../autoencoders/group-2024-05-14_chess/\"]\n",
    "# # autoencoder_group_paths = [\"../autoencoders/chess_layer0/\"]\n",
    "\n",
    "# csv_results_file = \"../autoencoders/chess_layer5_large_sweep/results.csv\"\n",
    "# # csv_results_file = \"../autoencoders/chess_layer0/results.csv\"\n",
    "# csv_results_file = \"../autoencoders/group-2024-05-14_chess/results.csv\"\n",
    "# output_path = csv_results_file.replace(\".csv\", \"_f1_results.csv\")\n",
    "\n",
    "\n",
    "# results_filename_filter = (\n",
    "#     \"1000\"  # This is only necessary if you have multiple files with multiple n_inputs\n",
    "# )\n",
    "# e.g. indexing_find_dots_indices_n_inputs_1000_results.pkl and indexing_find_dots_indices_n_inputs_5000_results.pkl\n",
    "# In this case, if you want to view the results for n_inputs = 1000, you would set filter = \"1000\"\n",
    "\n",
    "# custom_functions = get_custom_functions(autoencoder_group_paths[0], results_filename_filter, device)\n",
    "# custom_function_names = get_custom_function_names(custom_functions)\n",
    "# all_sae_results = get_all_sae_f1_results(autoencoder_group_paths, df, results_filename_filter, custom_functions, custom_function_names, device, thresholds, mask)\n",
    "\n",
    "# df = pd.read_csv(csv_results_file)\n",
    "# df = update_dataframe_with_results(df, all_sae_results, custom_function_names, autoencoder_group_paths, thresholds)\n",
    "# df.to_csv(output_path, index=False)\n",
    "\n",
    "# output_path = f1_analysis.complete_analysis_pipeline(autoencoder_group_paths, csv_results_file, results_filename_filter, device, thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = \"../autoencoders/group-2024-05-14_chess/chess_5_23_f1_results.csv\"\n",
    "# output_path = \"../autoencoders/othello_5-21/othello_f1_results.csv\"\n",
    "# output_path = \"../autoencoders/othello_f1_results_5-26.csv\"\n",
    "# output_path = \"../autoencoders/chess-trained_model-layer_5-2024-05-23/f1_results.csv\"\n",
    "output_path = \"../autoencoders/chess-trained_model-layer_5-2024-05-23-f1_results.csv\"\n",
    "# output_path = \"../autoencoders/chess-random_model-layer_5-2024-05-23-f1_results.csv\"\n",
    "# output_path = \"../autoencoders/othello-random_model-layer_5-2024-05-23-f1_results.csv\"\n",
    "# output_path = \"../autoencoders/chess-trained_model-layer_0-2024-05-23-f1_results.csv\"\n",
    "# output_path = \"../autoencoders/othello-trained_model-layer_0-2024-05-23-f1_results.csv\"\n",
    "# output_path = \"../autoencoders/othello-trained_model-layer_5-2024-05-23-f1_results.csv\"\n",
    "# output_path = \"../autoencoders/chess_f1_results_5-25_v2.csv\"\n",
    "# output_path = \"../autoencoders/othello_layer0/f1_results.csv\"\n",
    "#output_path = \"../autoencoders/othello_mlp_acts_identity_aes/results.csv\"\n",
    "output_path = \"../autoencoders/othello-6-10-all-layers-f1_results.csv\"\n",
    "output_path = \"../autoencoders/othello-6-10-mlp-acts-f1_results.csv\"\n",
    "# output_path = \"../autoencoders/othello-6-10-random-f1_results.csv\"\n",
    "# output_path = \"../autoencoders/othello-6-10-trained-f1_results.csv\"\n",
    "# output_path = \"../autoencoders/06-10-chess-all-layers-results.csv\"\n",
    "output_path = \"../autoencoders/06-10-chess-mlp-act-results.csv\"\n",
    "# output_path = \"../autoencoders/06-10-chess-random-results.csv\"\n",
    "# output_path = \"../autoencoders/06-10-chess-trained-results.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(output_path)\n",
    "def convert_json(x):\n",
    "    try:\n",
    "        return json.loads(x)  # Attempt to parse JSON\n",
    "    except (ValueError, TypeError):\n",
    "        return x  # Return the original value if it's not a JSON string\n",
    "\n",
    "def convert_dataframe(df):\n",
    "    # Apply the conversion function to each element in the DataFrame\n",
    "    return df.map(convert_json)\n",
    "df = convert_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "othello = f1_analysis.check_df_if_othello(df)\n",
    "print(f\"Is this Othello? {othello}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following coverage metric is \"for best f1 feature per board state property, is the f1 above `desired_threshold`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_threshold_columns = []\n",
    "coverage_threshold_idx_columns = []\n",
    "desired_threshold = 0.8\n",
    "for col in df.columns:\n",
    "    if f\"f1_threshold_{desired_threshold}_best_normalized\" in col and \"idx\" not in col:\n",
    "        coverage_threshold_columns.append(col)\n",
    "        print(col)\n",
    "    if f\"f1_threshold_{desired_threshold}_best_normalized_idx\" in col:\n",
    "        coverage_threshold_idx_columns.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is coverage with average f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "average_coverage_columns = []\n",
    "average_coverage_idx_columns = []\n",
    "\n",
    "average_coverage_key = \"best_average_f1\"\n",
    "\n",
    "for col in df.columns: \n",
    "    if average_coverage_key in col and \"idx\" not in col:\n",
    "        average_coverage_columns.append(col)\n",
    "        print(col)\n",
    "    if f\"{average_coverage_key}_idx\" in col:\n",
    "        average_coverage_idx_columns.append(col)\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is board reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_reconstruction_columns = []\n",
    "board_reconstruction_idx_columns = []\n",
    "\n",
    "board_reconstruction_key = \"best_f1_score_per_class\"\n",
    "\n",
    "for col in df.columns:\n",
    "    if board_reconstruction_key in col and \"idx\" not in col:\n",
    "        board_reconstruction_columns.append(col)\n",
    "        print(col)\n",
    "        f1_idx = col.replace(board_reconstruction_key, \"best_idx\")\n",
    "        board_reconstruction_idx_columns.append(f1_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell if you are checking out cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos_sim_columns = []\n",
    "\n",
    "# for col in df.columns:\n",
    "#     if \"cos_sim\" in col:\n",
    "#         cos_sim_columns.append(col)\n",
    "#         print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 2 cells find the average f1 score and custom metric score for all functions, all 8x8 board state functions, and all binary functions, then store it in the df and `average_metric_columns` and `average_metric_idx_columns`. It's pretty verbose, but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_average_columns = []\n",
    "paper_average_idx_columns = []\n",
    "\n",
    "chess_strategy_columns = [\n",
    "    (chess_utils.board_to_threat_state.__name__, 1.0),\n",
    "    (chess_utils.board_to_legal_moves_state.__name__, 1.0),\n",
    "    (chess_utils.board_to_check_state.__name__, 1.0),\n",
    "    (chess_utils.board_to_can_check_next.__name__, 1.0),\n",
    "    (chess_utils.board_to_has_queen.__name__, 1.0),\n",
    "    (chess_utils.board_to_can_capture_queen.__name__, 1.0),\n",
    "    (chess_utils.board_to_has_bishop_pair.__name__, 1.0),\n",
    "    (chess_utils.board_to_has_castling_rights.__name__, 1.0),\n",
    "    (chess_utils.board_to_has_kingside_castling_rights.__name__, 1.0),\n",
    "    (chess_utils.board_to_has_queenside_castling_rights.__name__, 1.0),\n",
    "    (chess_utils.board_to_any_fork.__name__, 1.0),\n",
    "    (chess_utils.board_to_pin_state.__name__, 1.0),\n",
    "    (chess_utils.board_to_has_legal_en_passant.__name__, 1.0),\n",
    "    (chess_utils.board_to_ambiguous_moves.__name__, 1.0),\n",
    "]\n",
    "\n",
    "# othello_low_level_columns = [\n",
    "#     (\"games_batch_to_state_stack_mine_yours_blank_mask_BLRRC\", 2.0),\n",
    "#     (\"games_batch_to_valid_moves_BLRRC\", 1.0),\n",
    "# ]\n",
    "\n",
    "if not othello:\n",
    "\n",
    "    df, paper_average_columns, paper_average_idx_columns = (\n",
    "        f1_analysis.add_average_high_level_board_reconstruction_for_columns(\n",
    "            df,\n",
    "            paper_average_columns,\n",
    "            paper_average_idx_columns,\n",
    "            chess_strategy_columns,\n",
    "            \"strategy_\",\n",
    "            board_reconstruction_key,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df, paper_average_columns, paper_average_idx_columns = (\n",
    "        f1_analysis.add_average_coverage_for_columns(\n",
    "            df,\n",
    "            paper_average_columns,\n",
    "            paper_average_idx_columns,\n",
    "            chess_strategy_columns,\n",
    "            \"strategy_\",\n",
    "            average_coverage_key,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paper_average_columns)\n",
    "print(paper_average_idx_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df\n",
    "# df.to_csv(\"processed_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select only the numerical columns\n",
    "# numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "# numerical_data = df[numerical_columns]\n",
    "\n",
    "# # calculate the correlation matrix\n",
    "# correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# # create a heatmap using plotly\n",
    "# fig = px.imshow(correlation_matrix, \n",
    "#                 labels=dict(x=\"Columns\", y=\"Columns\", color=\"Correlation\"),\n",
    "#                 x=correlation_matrix.columns,\n",
    "#                 y=correlation_matrix.columns,\n",
    "#                 color_continuous_scale='RdBu_r',\n",
    "#                 zmin=-1, zmax=1)\n",
    "\n",
    "# # update the layout\n",
    "# fig.update_layout(\n",
    "#     title='Correlation Matrix',\n",
    "#     width=2000,\n",
    "#     height=2000\n",
    "# )\n",
    "\n",
    "# # display the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top values obtained (used for paper results section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_n_values(df: pd.DataFrame, key: str, n: int, print_names: bool = False):\n",
    "    top_n_values = df.sort_values(by=key, ascending=False).head(n)\n",
    "    print(f\"Top {n} values for {key}\")\n",
    "    print(top_n_values[key])\n",
    "\n",
    "    if print_names:\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        print(top_n_values[\"autoencoder_path\"])\n",
    "        print(top_n_values[\"num_alive_features\"])\n",
    "\n",
    "    print()\n",
    "\n",
    "n = 30\n",
    "\n",
    "if othello:\n",
    "    reconstruction_key = 'games_batch_to_state_stack_mine_yours_blank_mask_BLRRC_best_f1_score_per_class'\n",
    "    reconstruction_key2 = 'games_batch_to_state_stack_mine_yours_blank_mask_BLRRC_last_f1_score_per_class'\n",
    "    coverage_key = 'games_batch_to_state_stack_mine_yours_blank_mask_BLRRC_best_average_f1'\n",
    "\n",
    "    print_top_n_values(df, reconstruction_key, n, print_names=True)\n",
    "    print_top_n_values(df, reconstruction_key2, n, print_names=True)\n",
    "    print_top_n_values(df, coverage_key, n, print_names=True)\n",
    "else:\n",
    "    reconstruction_key = \"board_to_piece_masked_blank_and_initial_state_best_f1_score_per_class\"\n",
    "    reconstruction_key2 = \"board_to_piece_masked_blank_and_initial_state_last_f1_score_per_class\"\n",
    "    coverage_key = \"board_to_piece_masked_blank_and_initial_state_best_average_f1\"\n",
    "    print_top_n_values(df, reconstruction_key, n, print_names=True)\n",
    "    print_top_n_values(df, reconstruction_key2, n, print_names=True)\n",
    "    print_top_n_values(df, coverage_key, n)\n",
    "    for key in chess_strategy_columns:\n",
    "        print_top_n_values(df, key[0] + \"_best_average_f1\", n)\n",
    "        print_top_n_values(df, key[0] + \"_best_f1_score_per_class\", n)\n",
    "        print_top_n_values(df, key[0] + \"_last_f1_score_per_class\", n)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# get unique trainer types\n",
    "unique_trainers = df[\"trainer_class\"].unique()\n",
    "\n",
    "# create a dictionary mapping trainer types to marker shapes\n",
    "trainer_markers = dict(zip(unique_trainers, [\"o\", \"X\", \"^\", \"d\"]))\n",
    "\n",
    "# “Gated SAE”, “Gated SAE w/ p-annealing”, “Standard”, “Standard w/ p-annealing”\n",
    "label_lookup = {\n",
    "    \"StandardTrainer\": \"Standard\",\n",
    "    \"PAnnealTrainer\": \"Standard w/ p-annealing\",\n",
    "    \"GatedSAETrainer\": \"Gated SAE\",\n",
    "    \"GatedAnnealTrainer\": \"Gated SAE w/ p-annealing\",\n",
    "    \"Identity\": \"Identity\",\n",
    "}\n",
    "\n",
    "metric_1_label = r'$L_0$ (Lower is sparser)'\n",
    "metric_2_label = 'Loss Recovered (Fidelity)'\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 18,  # Adjust base font size as needed\n",
    "    #'axes.titlesize': 22,  # Title font size\n",
    "    'axes.labelsize': 18,  # Axis labels font size\n",
    "    'xtick.labelsize': 18,  # X-axis tick labels font size\n",
    "    'ytick.labelsize': 18,  # Y-axis tick labels font size\n",
    "    'legend.fontsize': 16,  # Legend font size\n",
    "})\n",
    "\n",
    "\n",
    "def plot_l0_metric(\n",
    "    column_name: str,\n",
    "    y_label: str,\n",
    "    legend_location: str = \"lower right\",\n",
    "    xlims: tuple[float, float] = (0.0, 400.0),\n",
    "    full_title: Optional[str] = None,\n",
    "    output_filename: Optional[str] = None,\n",
    "):\n",
    "    # Create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Example data loading and processing code here\n",
    "    # Assuming df is your DataFrame and it's already defined/imported\n",
    "\n",
    "    metric_1 = \"l0\"\n",
    "    idx = df[column_name].values[0]  # Example of accessing a DataFrame\n",
    "\n",
    "    # Plot data points for each trainer type separately\n",
    "    for i, (trainer, marker) in enumerate(trainer_markers.items()):\n",
    "        trainer_data = df[df[\"trainer_class\"].str.contains(trainer)]\n",
    "# #         \"StandardTrainer\"\n",
    "# #         \"PAnnealTrainer\"\n",
    "# #         \"GatedSAETrainer\"\n",
    "# #         \"GatedAnnealTrainer\"\n",
    "#         alpha = 0.3\n",
    "# #         if trainer == \"GatedAnnealTrainer\":\n",
    "# #             alpha = 1.0\n",
    "        ax.scatter(\n",
    "                trainer_data[metric_1],\n",
    "                trainer_data[column_name],\n",
    "                marker=marker,\n",
    "                c=plt.cm.Set1(i),\n",
    "                s=100,\n",
    "                label=label_lookup[trainer],\n",
    "                # alpha=alpha\n",
    "            )\n",
    "#         for dict_size in df[\"dict_size\"].unique()[1:]:\n",
    "#             print(\"Dict size: \", dict_size)\n",
    "#             trainer_data_d = trainer_data[trainer_data[\"dict_size\"] == dict_size].sort_values('l0')\n",
    "#             plt.plot(trainer_data_d[metric_1], trainer_data_d[column_name], color=plt.cm.Set1(i), alpha=alpha)\n",
    "        \n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(metric_1_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "#     if full_title:\n",
    "#         ax.set_title(full_title)\n",
    "#     else:\n",
    "#         ax.set_title(f\"{metric_1} vs. at threshold {idx} for {column_name}\")\n",
    "\n",
    "    # # Draw a red dashed line\n",
    "    # red_line = ax.axhline(y=0.99, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "    # # Add the red dashed line to the legend\n",
    "    # red_proxy = mlines.Line2D([], [], color='red', linestyle='--', linewidth=2, label='Linear probe baseline')\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # handles.append(red_proxy)  # add the red line as a handle for the legend\n",
    "\n",
    "    # Display the legend\n",
    "    #ax.legend(handles=handles, title=\"Trainer Type\", loc=legend_location)\n",
    "    ax.legend(handles=handles, loc=legend_location)\n",
    "\n",
    "\n",
    "    # Set x and y range\n",
    "    ax.set_xlim(*xlims)\n",
    "    # ax.set_ylim(0.0, )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if othello:\n",
    "    graph1_column_name = \"games_batch_to_state_stack_mine_yours_BLRRC_best_f1_score_per_square\"\n",
    "    graph2_column_name = (\n",
    "        \"games_batch_to_state_stack_mine_yours_BLRRC_f1_threshold_0.8_best_normalized\"\n",
    "    )\n",
    "else:\n",
    "    graph1_column_name = \"board_to_piece_masked_blank_and_initial_state_best_f1_score_per_square\"\n",
    "    graph2_column_name = \"board_to_piece_masked_blank_and_initial_state_best_average_f1\"\n",
    "# graph1_column_name = \"board_to_piece_masked_blank_state_best_f1_score_per_square\"\n",
    "# graph1_column_name = \"board_to_piece_masked_blank_and_initial_state_best_average_f1\"\n",
    "# graph1_column_name = \"board_to_piece_masked_blank_state_best_average_f1\"\n",
    "# graph1_column_name = \"board_to_piece_masked_blank_and_initial_state_f1_threshold_0.8_best_normalized\"\n",
    "\n",
    "# plot_l0_metric(\n",
    "    # graph1_column_name, full_title=\"Percentage of squares with F1 score above 0.8 vs. L0\"\n",
    "# )\n",
    "# plot_l0_metric(graph2_column_name, full_title=\"Average F1 score vs. L0\")\n",
    "# plot_l0_metric(cos_sim_columns[0], full_title=\"Cosine similarity vs. L0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3var_graph(\n",
    "    color_column: str,\n",
    "    idx_column_name: str,\n",
    "    legend_location: str = \"lower right\",\n",
    "    xlims: tuple[float, float] = (0.0, 400.0),\n",
    "    y_lims: tuple[float, float] = (0.985, 1.001),\n",
    "    colorbar_label: Optional[str] = None,\n",
    "    output_filename: Optional[str] = None,\n",
    "):\n",
    "    # create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # create a normalize object for color scaling\n",
    "    # color_column = 'board_to_can_capture_queen_best_custom_metric'\n",
    "    # color_column = 'board_to_piece_state_best_custom_metric'\n",
    "    # color_column = 'board_to_has_legal_en_passant_best_custom_metric'\n",
    "    # color_column = 'board_to_pin_state_best_custom_metric'\n",
    "    # color_column = custom_metric_columns[6]\n",
    "    norm = Normalize(vmin=df[color_column].min(), vmax=df[color_column].max())\n",
    "\n",
    "    metric_1 = \"l0\"\n",
    "    metric_2 = \"frac_recovered\"\n",
    "\n",
    "    idx = df[idx_column_name].values[0]\n",
    "\n",
    "    handles, labels = [], []\n",
    "    \n",
    "    # plot data points for each trainer type separately\n",
    "    for trainer, marker in trainer_markers.items():\n",
    "        trainer_data = df[df[\"trainer_class\"].str.contains(trainer)]\n",
    "        scatter = ax.scatter(\n",
    "            trainer_data[metric_1],\n",
    "            trainer_data[metric_2],\n",
    "            c=trainer_data[color_column],\n",
    "            cmap=\"viridis\",\n",
    "            marker=marker,\n",
    "            s=100,\n",
    "            label=label_lookup[trainer],\n",
    "            norm=norm,\n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "        \n",
    "        # custom legend stuff\n",
    "        _handle, _ = scatter.legend_elements(prop=\"sizes\")\n",
    "        _handle[0].set_markeredgecolor(\"black\")\n",
    "        _handle[0].set_markerfacecolor(\"white\")\n",
    "        _handle[0].set_markersize(10)\n",
    "        if marker == \"d\":\n",
    "            _handle[0].set_markersize(13)\n",
    "        handles += _handle\n",
    "        labels.append(label_lookup[trainer])\n",
    "\n",
    "    # add colorbar\n",
    "    cbar = fig.colorbar(ax.collections[0], ax=ax, label=colorbar_label)\n",
    "    # cbar.set_label(color_column)\n",
    "\n",
    "    # set labels and title\n",
    "    ax.set_xlabel(metric_1_label)\n",
    "    ax.set_ylabel(metric_2_label)\n",
    "#     if full_title is not None:\n",
    "#         ax.set_title(full_title)\n",
    "#     else:\n",
    "#         ax.set_title(f\"{metric_1} vs. {metric_2} at threshold {idx} for {color_column}\")\n",
    "\n",
    "    # addnd\n",
    "    #ax.legend(handles, labels, title=\"Trainer Type\", loc=legend_location)\n",
    "    ax.legend(handles, labels, loc=legend_location)\n",
    "    \n",
    "    # # set x range\n",
    "    ax.set_xlim(*xlims)\n",
    "    # ax.set_ylim(0.990, 1.001)\n",
    "    ax.set_ylim(*y_lims)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# if othello:\n",
    "#     graph1_column_name = \"games_batch_to_state_stack_mine_yours_BLRRC_best_f1_score_per_square\"\n",
    "# else:\n",
    "#     graph1_column_name = \"board_to_piece_masked_blank_and_initial_state_best_f1_score_per_square\"\n",
    "\n",
    "# idx_column_name = graph1_column_name.replace(\"f1_score_per_square\", \"idx\")\n",
    "\n",
    "# plot_3var_graph(graph1_column_name, idx_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not othello:\n",
    "    board_reconstruction_column_name = reconstruction_key\n",
    "    board_reconstruction_column_name_idx = reconstruction_key.replace(\n",
    "        \"best_f1_per_class\", \"best_idx\"\n",
    "    )\n",
    "    coverage_column_name = coverage_key\n",
    "    coverage_column_name_idx = f\"{coverage_key}_idx\"\n",
    "\n",
    "    plot_l0_metric(\n",
    "        board_reconstruction_column_name,\n",
    "        y_label=\"Chess Board State Reconstruction $F_1$\",\n",
    "        legend_location=\"lower left\",\n",
    "        full_title=\"\",\n",
    "        output_filename=\"chess_2var_low_reconstruct.png\",\n",
    "    )\n",
    "    plot_l0_metric(\n",
    "        coverage_column_name,\n",
    "        y_label=\"Coverage of Chess Board State\",\n",
    "        legend_location=\"lower left\",\n",
    "        full_title=\"\",\n",
    "        output_filename=\"chess_2var_low_coverage.png\",\n",
    "    )\n",
    "    plot_3var_graph(\n",
    "        board_reconstruction_column_name,\n",
    "        board_reconstruction_column_name_idx,\n",
    "        colorbar_label=\"Chess Board State Reconstruction $F_1$\",\n",
    "        output_filename=\"chess_3var_low_reconstruct.png\",\n",
    "    )\n",
    "    plot_3var_graph(\n",
    "        coverage_column_name,\n",
    "        coverage_column_name_idx,\n",
    "        colorbar_label=\"Coverage of Chess Board State\",\n",
    "        output_filename=\"chess_3var_low_coverage.png\",\n",
    "    )\n",
    "\n",
    "    average_coverage_column = paper_average_columns[1]\n",
    "    average_coverage_column_idx = paper_average_idx_columns[1]\n",
    "    average_board_reconstruction_column = paper_average_columns[0]\n",
    "    average_board_reconstruction_column_idx = paper_average_idx_columns[0]\n",
    "\n",
    "    plot_l0_metric(\n",
    "        average_coverage_column,\n",
    "        y_label=\"Coverage of Chess Strategy BSPs\",\n",
    "        full_title=\"\",\n",
    "        output_filename=\"chess_2var_high_coverage.png\",\n",
    "    )\n",
    "    plot_l0_metric(\n",
    "        average_board_reconstruction_column,\n",
    "        y_label=\"Chess Strategy BSP Reconstruction $F_1$\",\n",
    "        full_title=\"\",\n",
    "        output_filename=\"chess_2var_high_reconstruct.png\",\n",
    "    )\n",
    "    plot_3var_graph(\n",
    "        average_coverage_column,\n",
    "        average_coverage_column_idx,\n",
    "        colorbar_label=\"Coverage of Chess Strategy BSPs\",\n",
    "        output_filename=\"chess_3var_high_coverage.png\",\n",
    "    )\n",
    "    plot_3var_graph(\n",
    "        average_board_reconstruction_column,\n",
    "        average_board_reconstruction_column_idx,\n",
    "        colorbar_label=\"Chess Strategy BSP Reconstruction $F_1$\",\n",
    "        output_filename=\"chess_3var_high_reconstruct.png\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if othello:\n",
    "    board_reconstruction_column_name = (\n",
    "        \"games_batch_to_state_stack_mine_yours_blank_mask_BLRRC_best_f1_score_per_class\"\n",
    "    )\n",
    "    board_reconstruction_column_name_idx = (\n",
    "        \"games_batch_to_state_stack_mine_yours_blank_mask_BLRRC_best_idx\"\n",
    "    )\n",
    "    coverage_column_name = \"games_batch_to_state_stack_mine_yours_blank_mask_BLRRC_best_average_f1\"\n",
    "    coverage_column_name_idx = (\n",
    "        \"games_batch_to_state_stack_mine_yours_blank_mask_BLRRC_best_average_f1_idx\"\n",
    "    )\n",
    "\n",
    "    plot_l0_metric(\n",
    "        board_reconstruction_column_name,\n",
    "        y_label=\"Othello Board State Reconstruction $F_1$\",\n",
    "        full_title=\"\",\n",
    "        output_filename=\"othello_2var_reconstruct.png\",\n",
    "    )\n",
    "    plot_l0_metric(\n",
    "        coverage_column_name,\n",
    "        y_label=\"Coverage of Othello Board State\",\n",
    "        full_title=\"\",\n",
    "        output_filename=\"othello_2var_coverage.png\",\n",
    "    )\n",
    "    plot_3var_graph(\n",
    "        board_reconstruction_column_name,\n",
    "        board_reconstruction_column_name_idx,\n",
    "        y_lims=(0.996, 1.0005),\n",
    "        colorbar_label=\"Othello Board State Reconstruction $F_1$\",\n",
    "        output_filename=\"othello_3var_reconstruct.png\",\n",
    "    )\n",
    "    plot_3var_graph(\n",
    "        coverage_column_name,\n",
    "        coverage_column_name_idx,\n",
    "        y_lims=(0.996, 1.0005),\n",
    "        colorbar_label=\"Coverage of Othello Board State\",\n",
    "        output_filename=\"othello_3var_coverage.png\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dictionary_learning.dictionary import AutoEncoder\n",
    "# path_to_dict_size = {}\n",
    "# for t_dir in df[\"autoencoder_path\"]:\n",
    "#     trainer_id = t_dir.split(\"/\")[-2]\n",
    "#     with open(t_dir + \"/config.json\", \"r\") as f:\n",
    "#         config = json.load(f)\n",
    "#         try:\n",
    "#             dict_size = config[\"trainer\"][\"dict_size\"]\n",
    "#         except:\n",
    "#             # standard trainer does not store dict_size, so we have to got get it\n",
    "#             ae = AutoEncoder.from_pretrained(t_dir + \"/ae.pt\", device=\"cpu\")\n",
    "#             dict_size = ae.encoder.out_features\n",
    "#         path_to_dict_size[t_dir] = dict_size\n",
    "\n",
    "# df[\"dict_size\"] = df[\"autoencoder_path\"].map(path_to_dict_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the next 4 cells for exploratory analysis. They will create many plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, column_name in enumerate(paper_average_columns):\n",
    "#     idx_column_name = paper_average_idx_columns[i]\n",
    "#     plot_3var_graph(column_name, idx_column_name, full_title=f\"Coverage {column_name}\")\n",
    "#     plot_l0_metric(column_name, full_title=f\"Coverage {column_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, column_name in enumerate(average_coverage_columns):\n",
    "#     idx_column_name = average_coverage_idx_columns[i]\n",
    "#     plot_3var_graph(column_name, idx_column_name, full_title=f\"Coverage {column_name}\")\n",
    "#     plot_l0_metric(column_name, full_title=f\"Coverage {column_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, column_name in enumerate(board_reconstruction_columns):\n",
    "#     idx_column_name = board_reconstruction_idx_columns[i]\n",
    "#     plot_3var_graph(column_name, idx_column_name, full_title=f\"Board Reconstruction {column_name}\")\n",
    "#     plot_l0_metric(column_name, full_title=f\"Board Reconstruction {column_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, column_name in enumerate(coverage_threshold_columns):\n",
    "#     idx_column_name = coverage_threshold_idx_columns[i]\n",
    "#     plot_3var_graph(column_name, idx_column_name, full_title=f\"Coverage {column_name}\")\n",
    "#     plot_l0_metric(column_name, full_title=f\"Coverage {column_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
